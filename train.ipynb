{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from or_gym.utils import create_env\n",
    "from gym.spaces import Box\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_name, n_envs, monitor_path, env_seed):\n",
    "    if n_envs == 1:\n",
    "        env = or_gym.make(env_name, seed_int=env_seed)\n",
    "        monitored_env = Monitor(env, monitor_path)\n",
    "        return monitored_env\n",
    "    else:\n",
    "        def make_env():    \n",
    "            def _init():\n",
    "                return or_gym.make(env_name)\n",
    "            return _init\n",
    "    \n",
    "        vec_env = make_vec_env(make_env(), n_envs=n_envs, seed=env_seed, vec_env_cls=DummyVecEnv)\n",
    "        #vec_env = make_vec_env(env, n_envs, env_seed, vec_env_cls=DummyVecEnv)\n",
    "        monitored_vec_env = VecMonitor(vec_env, monitor_path)\n",
    "        return monitored_vec_env\n",
    "\n",
    "\n",
    "def train_model_on_env(env_name, algorithm, policy, name='default', n_envs=4, env_seed=42, timesteps=100000, eval_freq=1000):\n",
    "    env_model_path = f'./data/{env_name}/{algorithm.__name__}/{name}/'\n",
    "\n",
    "    if not os.path.exists(env_model_path):\n",
    "        os.makedirs(env_model_path)\n",
    "\n",
    "    env = create_env(env_name, n_envs, env_model_path, env_seed)\n",
    "    model = algorithm(policy, env)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=env_model_path, verbose=1,\n",
    "                                log_path=env_model_path, eval_freq=eval_freq, deterministic=True, render=False)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-268.81 +/- 5.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-272.40 +/- 5.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-269.01 +/- 3.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-270.07 +/- 2.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-258.28 +/- 2.37\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-255.51 +/- 6.79\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=-241.10 +/- 2.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=-237.15 +/- 3.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', PPO, PPOPolicy, timesteps=3000000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=20.66 +/- 2.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=183.58 +/- 9.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=178.69 +/- 3.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=151.16 +/- 7.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=199.47 +/- 5.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=192.21 +/- 7.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=266.38 +/- 7.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=275.29 +/- 16.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=423.81 +/- 13.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=408.19 +/- 9.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=397.52 +/- 15.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=243.98 +/- 6.56\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=416.34 +/- 11.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=265.06 +/- 5.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=363.26 +/- 3.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=372.16 +/- 6.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=321.29 +/- 19.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=366.83 +/- 5.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=300.26 +/- 8.57\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=301.27 +/- 8.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=321.10 +/- 5.25\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=302.35 +/- 12.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=336.24 +/- 7.07\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=410.02 +/- 21.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=416.53 +/- 17.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=387.41 +/- 9.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=305.06 +/- 10.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=320.70 +/- 12.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=281.46 +/- 10.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=309.01 +/- 12.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=366.61 +/- 15.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=241.94 +/- 8.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=315.60 +/- 17.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=246.09 +/- 16.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=255.60 +/- 16.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=388.29 +/- 11.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=301.10 +/- 13.38\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=252.82 +/- 6.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=356.25 +/- 8.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=380.18 +/- 12.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=388.85 +/- 19.55\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=418.21 +/- 13.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=187.57 +/- 9.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=290.11 +/- 3.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=279.09 +/- 9.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=290.79 +/- 9.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=258.54 +/- 10.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=310.73 +/- 6.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=334.77 +/- 11.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=391.43 +/- 5.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=279.04 +/- 18.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=335.96 +/- 4.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=354.20 +/- 11.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=308.93 +/- 12.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=255.30 +/- 7.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=356.56 +/- 6.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=300.83 +/- 16.49\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=385.45 +/- 11.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=379.64 +/- 17.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=309.24 +/- 6.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=424.67 +/- 14.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2480000, episode_reward=421.75 +/- 14.94\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', A2C, A2CPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=196.95 +/- 28.86\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=210.65 +/- 22.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=181.60 +/- 36.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=103.82 +/- 24.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=66.20 +/- 33.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=54.12 +/- 12.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=151.05 +/- 32.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=217.17 +/- 42.04\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=286.09 +/- 16.69\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=249.35 +/- 49.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=248.10 +/- 18.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=185.99 +/- 31.11\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=241.60 +/- 25.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=244.34 +/- 63.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=216.80 +/- 20.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=195.80 +/- 51.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=250.58 +/- 23.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=210.10 +/- 50.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=170.95 +/- 39.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=192.70 +/- 10.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=199.64 +/- 24.49\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=204.76 +/- 12.69\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=259.16 +/- 25.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=193.68 +/- 18.89\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=216.66 +/- 36.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=186.85 +/- 23.79\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=231.29 +/- 34.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=254.91 +/- 13.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=252.63 +/- 32.25\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=254.73 +/- 43.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=174.05 +/- 32.48\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=197.20 +/- 58.35\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=196.91 +/- 11.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=212.65 +/- 23.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=198.89 +/- 22.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=228.14 +/- 26.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=244.33 +/- 3.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=273.84 +/- 14.60\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=266.14 +/- 44.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=254.30 +/- 32.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=294.72 +/- 20.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1680000, episode_reward=313.74 +/- 21.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1720000, episode_reward=237.16 +/- 28.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=298.90 +/- 27.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=260.13 +/- 24.07\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=214.20 +/- 25.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=200.60 +/- 21.09\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=218.58 +/- 50.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=210.79 +/- 36.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=236.57 +/- 27.07\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=229.00 +/- 29.06\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=250.27 +/- 35.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=246.48 +/- 37.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=220.85 +/- 36.20\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=263.39 +/- 22.03\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=265.06 +/- 28.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=255.60 +/- 29.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=251.89 +/- 28.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=240.87 +/- 46.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=226.15 +/- 31.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=221.78 +/- 68.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=249.66 +/- 22.06\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', SAC, SACPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=287.97 +/- 31.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=301.76 +/- 37.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=364.25 +/- 32.09\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=361.79 +/- 19.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=393.80 +/- 33.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=378.23 +/- 24.59\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=408.09 +/- 23.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=396.41 +/- 50.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=398.58 +/- 23.42\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=374.94 +/- 18.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=400.04 +/- 36.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=421.77 +/- 8.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520000, episode_reward=408.96 +/- 22.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=416.83 +/- 22.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=399.96 +/- 32.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=395.76 +/- 32.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=405.24 +/- 19.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=409.48 +/- 6.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=397.04 +/- 11.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=428.15 +/- 13.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840000, episode_reward=409.69 +/- 32.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=397.70 +/- 28.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=376.15 +/- 24.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=410.94 +/- 26.79\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=390.19 +/- 23.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=424.65 +/- 18.09\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=410.96 +/- 18.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=420.85 +/- 24.38\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=426.04 +/- 24.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=427.99 +/- 19.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=403.08 +/- 16.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=416.41 +/- 29.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=429.47 +/- 22.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1360000, episode_reward=436.70 +/- 13.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=431.23 +/- 19.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=418.06 +/- 11.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=428.16 +/- 15.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=427.64 +/- 6.73\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=427.01 +/- 30.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=412.34 +/- 24.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=420.75 +/- 29.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=426.41 +/- 20.58\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=438.53 +/- 17.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1760000, episode_reward=442.22 +/- 30.35\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=442.95 +/- 22.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1840000, episode_reward=423.82 +/- 24.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=433.75 +/- 20.25\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=422.98 +/- 16.21\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=445.49 +/- 21.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000000, episode_reward=428.77 +/- 14.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=427.32 +/- 17.33\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=405.27 +/- 28.15\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=415.69 +/- 22.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=421.79 +/- 28.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=420.83 +/- 22.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=432.18 +/- 10.49\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=417.25 +/- 13.33\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=425.63 +/- 22.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=450.32 +/- 12.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2400000, episode_reward=423.12 +/- 17.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=431.29 +/- 13.58\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=414.19 +/- 23.03\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', ARS, ARSPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=4000, episode_reward=184.73 +/- 28.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=282.95 +/- 27.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=322.64 +/- 24.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=284.00 +/- 13.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=249.72 +/- 9.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=75.37 +/- 14.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-96.91 +/- 7.72\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_on_env(\u001b[39m'\u001b[39;49m\u001b[39mInvManagement-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, TQC, TQCPolicy, timesteps\u001b[39m=\u001b[39;49m\u001b[39m80000\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algorithm, policy, name, n_envs, env_seed, timesteps, eval_freq)\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[39m=\u001b[39m algorithm(policy, env)\n\u001b[1;32m     27\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39menv_model_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m                             log_path\u001b[39m=\u001b[39menv_model_path, eval_freq\u001b[39m=\u001b[39meval_freq, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/tqc/tqc.py:296\u001b[0m, in \u001b[0;36mTQC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39m: SelfTQC,\n\u001b[1;32m    288\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    294\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTQC:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    297\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    298\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    299\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    300\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    301\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    302\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    303\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:334\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    331\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    333\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 334\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    336\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    337\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    338\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    339\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    340\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    341\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:567\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    564\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    566\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    569\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    570\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:76\u001b[0m, in \u001b[0;36mVecMonitor.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m---> 76\u001b[0m     obs, rewards, dones, infos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvenv\u001b[39m.\u001b[39;49mstep_wait()\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_returns \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/or_gym/envs/supply_chain/inventory_management.py:389\u001b[0m, in \u001b[0;36mInvManagementMasterEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_STEP(action)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/or_gym/envs/supply_chain/inventory_management.py:332\u001b[0m, in \u001b[0;36mInvManagementMasterEnv._STEP\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    330\u001b[0m II \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(I,\u001b[39m0\u001b[39m) \u001b[39m# augment inventory so that last has no onsite inventory\u001b[39;00m\n\u001b[1;32m    331\u001b[0m RR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(R,S[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m# augment replenishment orders to include production cost at last stage\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m P \u001b[39m=\u001b[39m a\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mn\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49msum(p\u001b[39m*\u001b[39;49mS \u001b[39m-\u001b[39;49m (r\u001b[39m*\u001b[39;49mRR \u001b[39m+\u001b[39;49m k\u001b[39m*\u001b[39;49mU \u001b[39m+\u001b[39;49m h\u001b[39m*\u001b[39;49mII)) \u001b[39m# discounted profit in period n\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[39m# P = a**n*np.sum(p*S - (r*RR + k*U + h*I))\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mP[n] \u001b[39m=\u001b[39m P \u001b[39m# store P\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', TQC, TQCPolicy, timesteps=80000, eval_freq=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-270.87 +/- 2.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-251.62 +/- 4.14\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-163.15 +/- 6.09\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-66.92 +/- 5.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=5.14 +/- 6.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=40.12 +/- 4.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=45.02 +/- 3.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=111.62 +/- 3.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=173.64 +/- 10.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=275.08 +/- 10.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=334.55 +/- 10.05\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=361.53 +/- 6.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520000, episode_reward=376.86 +/- 7.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=374.18 +/- 12.78\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=389.23 +/- 17.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=640000, episode_reward=387.63 +/- 18.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=382.51 +/- 6.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=397.97 +/- 31.35\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=760000, episode_reward=412.36 +/- 6.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=423.59 +/- 18.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840000, episode_reward=426.36 +/- 11.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=419.82 +/- 10.87\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=424.14 +/- 14.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=409.08 +/- 26.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=414.45 +/- 19.71\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=431.22 +/- 8.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1080000, episode_reward=432.82 +/- 13.29\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1120000, episode_reward=427.71 +/- 10.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=428.31 +/- 9.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=418.75 +/- 10.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=410.39 +/- 28.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=427.79 +/- 16.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=423.91 +/- 15.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=436.76 +/- 9.95\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=445.78 +/- 8.49\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1440000, episode_reward=433.26 +/- 18.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=442.30 +/- 9.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=432.65 +/- 22.04\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=449.01 +/- 17.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1600000, episode_reward=436.74 +/- 24.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=419.31 +/- 21.06\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=450.68 +/- 9.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1720000, episode_reward=426.93 +/- 12.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=432.15 +/- 23.22\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=425.39 +/- 15.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=450.36 +/- 13.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=456.15 +/- 18.01\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1920000, episode_reward=441.28 +/- 16.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=447.55 +/- 32.37\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=454.63 +/- 17.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=454.88 +/- 17.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=445.01 +/- 22.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=436.07 +/- 23.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=429.76 +/- 26.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=442.40 +/- 15.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=449.30 +/- 20.28\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=435.75 +/- 12.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=446.96 +/- 19.17\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=443.18 +/- 14.55\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=440.60 +/- 19.21\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=455.48 +/- 16.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=453.65 +/- 24.23\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', TRPO, TRPOPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-213.86 +/- 4.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=12.62 +/- 3.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=208.88 +/- 8.19\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=295.88 +/- 11.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=340.69 +/- 11.06\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=355.44 +/- 8.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=406.45 +/- 9.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=414.00 +/- 8.25\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=419.55 +/- 17.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=432.30 +/- 10.93\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=423.44 +/- 6.33\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=427.54 +/- 11.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=439.53 +/- 14.54\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=435.54 +/- 21.01\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=445.52 +/- 14.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=640000, episode_reward=431.91 +/- 42.12\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=439.18 +/- 17.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=458.48 +/- 9.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=760000, episode_reward=415.07 +/- 25.56\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=446.41 +/- 14.59\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_on_env(\u001b[39m'\u001b[39;49m\u001b[39mInvManagement-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, RecurrentPPO, RecurrentActorCriticPolicy, timesteps\u001b[39m=\u001b[39;49m\u001b[39m2500000\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algorithm, policy, name, n_envs, env_seed, timesteps, eval_freq)\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[39m=\u001b[39m algorithm(policy, env)\n\u001b[1;32m     27\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39menv_model_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m                             log_path\u001b[39m=\u001b[39menv_model_path, eval_freq\u001b[39m=\u001b[39meval_freq, deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mtimesteps, callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:487\u001b[0m, in \u001b[0;36mRecurrentPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mrecord(\u001b[39m\"\u001b[39m\u001b[39mtime/total_timesteps\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps, exclude\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtensorboard\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    485\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdump(step\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps)\n\u001b[0;32m--> 487\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    489\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/ppo_recurrent/ppo_recurrent.py:347\u001b[0m, in \u001b[0;36mRecurrentPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_sde:\n\u001b[1;32m    345\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mreset_noise(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m--> 347\u001b[0m values, log_prob, entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mevaluate_actions(\n\u001b[1;32m    348\u001b[0m     rollout_data\u001b[39m.\u001b[39;49mobservations,\n\u001b[1;32m    349\u001b[0m     actions,\n\u001b[1;32m    350\u001b[0m     rollout_data\u001b[39m.\u001b[39;49mlstm_states,\n\u001b[1;32m    351\u001b[0m     rollout_data\u001b[39m.\u001b[39;49mepisode_starts,\n\u001b[1;32m    352\u001b[0m )\n\u001b[1;32m    354\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    355\u001b[0m \u001b[39m# Normalize advantage\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/common/recurrent/policies.py:334\u001b[0m, in \u001b[0;36mRecurrentActorCriticPolicy.evaluate_actions\u001b[0;34m(self, obs, actions, lstm_states, episode_starts)\u001b[0m\n\u001b[1;32m    332\u001b[0m latent_pi, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_sequence(pi_features, lstm_states\u001b[39m.\u001b[39mpi, episode_starts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_actor)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm_critic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m     latent_vf, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_sequence(vf_features, lstm_states\u001b[39m.\u001b[39;49mvf, episode_starts, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm_critic)\n\u001b[1;32m    335\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshared_lstm:\n\u001b[1;32m    336\u001b[0m     latent_vf \u001b[39m=\u001b[39m latent_pi\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/common/recurrent/policies.py:200\u001b[0m, in \u001b[0;36mRecurrentActorCriticPolicy._process_sequence\u001b[0;34m(features, lstm_states, episode_starts, lstm)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m# Iterate over the sequence\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mfor\u001b[39;00m features, episode_start \u001b[39min\u001b[39;00m zip_strict(features_sequence, episode_starts):\n\u001b[0;32m--> 200\u001b[0m     hidden, lstm_states \u001b[39m=\u001b[39m lstm(\n\u001b[1;32m    201\u001b[0m         features\u001b[39m.\u001b[39;49munsqueeze(dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m),\n\u001b[1;32m    202\u001b[0m         (\n\u001b[1;32m    203\u001b[0m             \u001b[39m# Reset the states at the beginning of a new episode\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m             (\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m episode_start)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, n_seq, \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m lstm_states[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    205\u001b[0m             (\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m episode_start)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, n_seq, \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m lstm_states[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    206\u001b[0m         ),\n\u001b[1;32m    207\u001b[0m     )\n\u001b[1;32m    208\u001b[0m     lstm_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [hidden]\n\u001b[1;32m    209\u001b[0m \u001b[39m# Sequence to batch\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# (sequence length, n_seq, lstm_out_dim) -> (batch_size, lstm_out_dim)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/rnn.py:774\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    772\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    775\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    776\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    777\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    778\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', RecurrentPPO, RecurrentActorCriticPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('NetworkManagement-v1', PPO, PPOPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('NetworkManagement-v1', TRPO, TRPOPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('NetworkManagement-v1', RecurrentPPO, RecurrentActorCriticPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('NetworkManagement-v1', ARS, ARSPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('NetworkManagement-v1', A2C, A2CPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
