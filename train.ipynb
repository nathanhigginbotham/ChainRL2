{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from or_gym.utils import create_env\n",
    "from gym.spaces import Box\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(env_name, n_envs, monitor_path, env_seed):\n",
    "    if n_envs == 1:\n",
    "        env = or_gym.make(env_name, seed_int=env_seed)\n",
    "        monitored_env = Monitor(env, monitor_path)\n",
    "        return monitored_env\n",
    "    else:\n",
    "        def make_env():    \n",
    "            def _init():\n",
    "                return or_gym.make(env_name, seed_int=env_seed)\n",
    "            return _init\n",
    "    \n",
    "        vec_env = make_vec_env(make_env(), n_envs=n_envs, seed=env_seed, vec_env_cls=DummyVecEnv)\n",
    "        #vec_env = make_vec_env(env, n_envs, env_seed, vec_env_cls=DummyVecEnv)\n",
    "        monitored_vec_env = VecMonitor(vec_env, monitor_path)\n",
    "        return monitored_vec_env\n",
    "\n",
    "\n",
    "def train_model_on_env(env_name, algorithm, policy, name='default', n_envs=4, env_seed=42, timesteps=100000, eval_freq=1000):\n",
    "    env_model_path = f'./data/{env_name}/{algorithm.__name__}/{name}/'\n",
    "\n",
    "    if not os.path.exists(env_model_path):\n",
    "        os.makedirs(env_model_path)\n",
    "\n",
    "    env = create_env(env_name, n_envs, env_model_path, env_seed)\n",
    "    model = algorithm(policy, env)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=env_model_path, verbose=1,\n",
    "                                log_path=env_model_path, eval_freq=eval_freq, deterministic=True, render=False)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps, callback=eval_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=-268.81 +/- 5.45\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-272.40 +/- 5.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-269.01 +/- 3.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-257.26 +/- 1.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-250.25 +/- 1.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=-217.45 +/- 6.85\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=-185.44 +/- 4.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=-152.40 +/- 5.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=-120.23 +/- 5.79\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=-88.92 +/- 5.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=-60.84 +/- 6.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=480000, episode_reward=-34.04 +/- 7.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520000, episode_reward=-9.94 +/- 2.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=-6.53 +/- 3.02\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=10.27 +/- 9.76\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=640000, episode_reward=29.31 +/- 4.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=680000, episode_reward=42.21 +/- 3.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=720000, episode_reward=62.01 +/- 7.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=760000, episode_reward=77.32 +/- 9.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=800000, episode_reward=104.72 +/- 1.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=840000, episode_reward=126.36 +/- 4.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=123.82 +/- 5.08\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=137.45 +/- 2.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=960000, episode_reward=154.37 +/- 5.06\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=170.71 +/- 5.37\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1040000, episode_reward=182.20 +/- 10.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1080000, episode_reward=189.09 +/- 6.36\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1120000, episode_reward=205.03 +/- 13.09\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1160000, episode_reward=211.51 +/- 9.91\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1200000, episode_reward=226.71 +/- 4.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1240000, episode_reward=227.33 +/- 4.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1280000, episode_reward=241.23 +/- 4.58\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1320000, episode_reward=249.83 +/- 6.59\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1360000, episode_reward=259.72 +/- 7.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=264.26 +/- 4.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1440000, episode_reward=275.25 +/- 4.66\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1480000, episode_reward=274.90 +/- 12.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=274.48 +/- 16.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=281.76 +/- 15.98\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1600000, episode_reward=297.85 +/- 4.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1640000, episode_reward=307.56 +/- 10.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1680000, episode_reward=307.62 +/- 4.39\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1720000, episode_reward=307.74 +/- 9.12\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1760000, episode_reward=315.48 +/- 6.43\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=313.47 +/- 7.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=313.94 +/- 7.38\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=340.54 +/- 7.35\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1920000, episode_reward=343.21 +/- 4.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1960000, episode_reward=335.26 +/- 10.53\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=348.24 +/- 7.80\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2040000, episode_reward=348.07 +/- 8.70\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=357.27 +/- 7.14\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2120000, episode_reward=353.15 +/- 12.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=359.79 +/- 7.84\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2200000, episode_reward=361.78 +/- 16.74\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2240000, episode_reward=366.22 +/- 15.15\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2280000, episode_reward=376.05 +/- 5.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2320000, episode_reward=368.83 +/- 9.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=387.47 +/- 7.42\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2400000, episode_reward=378.29 +/- 16.74\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=385.40 +/- 10.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=379.75 +/- 16.05\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', PPO, PPOPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=20.66 +/- 2.67\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=183.58 +/- 9.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=178.69 +/- 3.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=151.16 +/- 7.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=199.47 +/- 5.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=240000, episode_reward=192.21 +/- 7.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=266.38 +/- 7.81\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=275.29 +/- 16.92\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=423.81 +/- 13.63\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=408.19 +/- 9.62\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=397.52 +/- 15.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=243.98 +/- 6.56\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=416.34 +/- 11.88\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=265.06 +/- 5.41\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=363.26 +/- 3.99\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=372.16 +/- 6.16\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=321.29 +/- 19.10\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=366.83 +/- 5.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=300.26 +/- 8.57\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=301.27 +/- 8.77\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=321.10 +/- 5.25\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=302.35 +/- 12.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=336.24 +/- 7.07\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=410.02 +/- 21.97\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=416.53 +/- 17.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=387.41 +/- 9.94\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=305.06 +/- 10.52\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=320.70 +/- 12.75\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=281.46 +/- 10.26\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=309.01 +/- 12.34\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=366.61 +/- 15.24\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=241.94 +/- 8.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=315.60 +/- 17.51\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=246.09 +/- 16.65\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=255.60 +/- 16.00\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=388.29 +/- 11.31\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=301.10 +/- 13.38\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=252.82 +/- 6.64\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=356.25 +/- 8.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=380.18 +/- 12.46\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=388.85 +/- 19.55\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=418.21 +/- 13.96\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=187.57 +/- 9.27\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=290.11 +/- 3.40\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=279.09 +/- 9.30\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=290.79 +/- 9.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=258.54 +/- 10.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=310.73 +/- 6.47\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=334.77 +/- 11.68\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=391.43 +/- 5.61\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=279.04 +/- 18.23\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=335.96 +/- 4.18\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=354.20 +/- 11.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=308.93 +/- 12.44\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=255.30 +/- 7.72\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=356.56 +/- 6.13\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=300.83 +/- 16.49\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=385.45 +/- 11.83\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=379.64 +/- 17.82\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=309.24 +/- 6.32\n",
      "Episode length: 30.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=424.67 +/- 14.90\n",
      "Episode length: 30.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2480000, episode_reward=421.75 +/- 14.94\n",
      "Episode length: 30.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env('InvManagement-v1', A2C, A2CPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env('InvManagement-v1', SAC, SACPolicy, timesteps=2500000, eval_freq=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
