{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "from common import make_env\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_env(env_name, algo_name, name, n_envs=1, timesteps=int(1e5), eval_freq=int(5e3), env_seed=0):\n",
    "    save_path = f'./data/{env_name}/{algo_name}/{name}/'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Using existing directory {save_path}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Creating new directory {save_path}')\n",
    "\n",
    "    def make_subproc_env():    \n",
    "        def _init():\n",
    "            make_env(env_name)\n",
    "        return _init\n",
    "    \n",
    "    if n_envs == 1:\n",
    "        env = make_env(env_name)\n",
    "        env = Monitor(env, save_path)\n",
    "    else:\n",
    "        env = SubprocVecEnv([make_subproc_env() for _ in range(n_envs)])\n",
    "        env = VecMonitor(env, save_path)\n",
    "\n",
    "    def make_model(algo_name, env, n_steps, batch_size):\n",
    "        if algo_name == 'PPO':\n",
    "            return PPO(PPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'RecurrentPPO':\n",
    "            return RecurrentPPO(RecurrentActorCriticPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'A2C':\n",
    "            return A2C(A2CPolicy, env, n_steps=n_steps)\n",
    "        \n",
    "        if algo_name == 'ARS':\n",
    "            return ARS(ARSPolicy, env)\n",
    "\n",
    "        if algo_name == 'SAC':\n",
    "            return SAC(SACPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TQC':\n",
    "            return TQC(TQCPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TRPO':\n",
    "            return TRPO(TRPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "    model = make_model(algo_name, env, n_steps=env.num_periods, batch_size=env.num_periods*n_envs)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=save_path, verbose=1, log_path=save_path, \n",
    "                                    eval_freq=int(eval_freq), deterministic=True, render=False)\n",
    "\n",
    "    model.learn(total_timesteps=int(timesteps), callback=eval_callback)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/TRPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1466.54 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1407.51 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1380.62 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1365.15 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-1367.34 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-793.46 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-188.12 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-454.58 +/- 3.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-173.95 +/- 2.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=167.51 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=277.26 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=428.61 +/- 2.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=531.72 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=452.48 +/- 1.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=484.41 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=643.13 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=647.98 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=525.94 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=889.05 +/- 5.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=873.25 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=977.15 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=986.09 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=900.15 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=959.23 +/- 6.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1130.49 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1143.90 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1122.68 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=816.19 +/- 24.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1009.15 +/- 3.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=591.41 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=378.58 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=110.11 +/- 3.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=645.87 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1215.40 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=1373.87 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=935.45 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=772.35 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=645.88 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=427.53 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=638.56 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1046.12 +/- 9.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=955.72 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=889.75 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=512.47 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=670.81 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=861.65 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=591.08 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=577.22 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=650.73 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=343.88 +/- 4.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=865.21 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=282.41 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=856.42 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1523.03 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=271.37 +/- 2.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=669.53 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=769.06 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=502.59 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=900.92 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=695.58 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=673.66 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=812.36 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=32.82 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=498.13 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=452.26 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=340.07 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=629.90 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=469.21 +/- 6.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=743.74 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=485.43 +/- 4.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=677.06 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=260.46 +/- 4.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=725.70 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=713.28 +/- 3.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=715.85 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=658.03 +/- 5.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=278.32 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=842.58 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=725.02 +/- 4.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=768.24 +/- 5.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1025.89 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1017.56 +/- 0.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=483.28 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=759.46 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=279.93 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=636.89 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=411.69 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1423.82 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1933.98 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=2349.22 +/- 7.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=455000, episode_reward=2231.58 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1648.08 +/- 3.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=767.44 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=801.43 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=781.98 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=616.30 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=262.05 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=829.96 +/- 5.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=358.04 +/- 4.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=635.31 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=165.28 +/- 1.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-75.93 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-52.45 +/- 5.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=239.75 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.17 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=73.51 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-414.65 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-362.95 +/- 4.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=44.72 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=401.99 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=755.70 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=897.54 +/- 8.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=896.07 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1013.88 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=942.50 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=256.83 +/- 2.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1316.10 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1166.36 +/- 6.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1391.68 +/- 2.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1373.90 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1026.87 +/- 4.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1098.62 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=888.81 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=666.64 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=438.93 +/- 1.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=437.20 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=474.35 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=566.64 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=639.23 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=415.32 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=641.87 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=562.10 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=557.43 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=288.16 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=286.54 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=524.90 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=478.29 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=211.83 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=425.61 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=319.29 +/- 3.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=243.11 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=568.24 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=128.29 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=246.72 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=38.46 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=32.86 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=37.48 +/- 4.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=22.30 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=39.21 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=183.76 +/- 3.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=266.24 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=235.65 +/- 4.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=43.90 +/- 1.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=558.81 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=590.55 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=643.77 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=813.64 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=979.12 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1075.83 +/- 5.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1080.87 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1059.86 +/- 8.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1093.41 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1172.74 +/- 10.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1204.08 +/- 9.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1242.25 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1234.60 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=800.62 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1189.78 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1052.57 +/- 1.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=930.66 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=540.89 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=988.44 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=930.63 +/- 1.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=839.07 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=713.47 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=817.28 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=669.67 +/- 4.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=648.91 +/- 2.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=955.16 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1067.55 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1079.90 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=869.46 +/- 1.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1026.67 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=797.96 +/- 4.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=733.70 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=530.73 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=879.70 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=637.50 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1062.22 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1148.41 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1334.87 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1534.03 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1484.27 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1494.62 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1458.73 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1619.50 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1706.35 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1642.67 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1668.64 +/- 6.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1609.13 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TRPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1523.62 +/- 4.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1521.26 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1292.16 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1415.39 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1408.21 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1484.25 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1401.90 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1096.19 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1134.17 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-905.83 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1187.99 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1045.14 +/- 1.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-797.44 +/- 1.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-507.18 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-470.74 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-177.33 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=71.78 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-24.12 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=132.01 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=267.62 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=410.23 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=436.50 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=290.61 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=510.28 +/- 3.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=646.54 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=972.33 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=929.72 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=721.92 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=324.63 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=759.06 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=648.35 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=672.73 +/- 2.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=500.33 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=497.71 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=716.17 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=646.17 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=70.45 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=539.16 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=563.71 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=414.14 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=320.99 +/- 1.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=524.77 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=549.86 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=581.23 +/- 3.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=518.25 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=486.43 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=528.24 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=621.63 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=559.82 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=511.57 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=487.11 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=327.44 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=768.25 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=802.01 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=736.80 +/- 1.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=851.61 +/- 2.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=786.26 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=889.21 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=682.65 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=647.63 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=621.33 +/- 3.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=311.20 +/- 2.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=674.11 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=833.60 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=497.28 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=637.74 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=677.05 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=413.18 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=424.77 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=507.36 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=591.16 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=647.70 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=557.29 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=598.59 +/- 4.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=830.00 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=742.52 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=584.90 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=529.74 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=516.88 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=582.70 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=522.81 +/- 7.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=438.39 +/- 2.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=521.55 +/- 1.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=577.26 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=680.27 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=630.15 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=470.67 +/- 2.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=489.37 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=446.07 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=659.82 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=423.34 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=746.86 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=776.05 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=630.62 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=549.38 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=671.48 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.93 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=682.85 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=579.33 +/- 0.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=740.48 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=858.86 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=964.12 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=766.47 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=681.28 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=586.94 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=556.03 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=550.89 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=462.81 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=315.58 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=715.75 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=528.14 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=463.00 +/- 2.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=701.41 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=750.82 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=515.96 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=607.09 +/- 1.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=530.09 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=195.37 +/- 2.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-11.73 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=660.02 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=689.09 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=522.97 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=728.94 +/- 6.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=613.46 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=754.02 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=673.61 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=750.66 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1115.33 +/- 3.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=645000, episode_reward=1172.74 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=1104.72 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=998.91 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=969.90 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1038.23 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=903.60 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=706.41 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=504.29 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=479.27 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=696.12 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=506.95 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=733.60 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=688.56 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=655.01 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=409.48 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=570.45 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=509.39 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=552.82 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=709.72 +/- 4.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=564.81 +/- 4.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=629.25 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=540.45 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=519.98 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=585.71 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=510.68 +/- 9.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=438.97 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=420.57 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=701.62 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=628.14 +/- 9.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=516.98 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=763.92 +/- 2.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=712.94 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=746.26 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=490.38 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=788.73 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=625.38 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=548.14 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=780.35 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=790.74 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=763.38 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=611.96 +/- 1.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=608.05 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=638.73 +/- 2.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=601.23 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=608.05 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=637.89 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=591.63 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=661.25 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=721.13 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=702.32 +/- 6.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=650.57 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=606.79 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=616.37 +/- 4.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=562.08 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=609.37 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=589.32 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=578.78 +/- 0.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=692.01 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=715.81 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=771.07 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=668.83 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=590.97 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=326.50 +/- 6.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=438.73 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=398.04 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=612.61 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=587.64 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=645.84 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=605.58 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=882.75 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=613.40 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=635.22 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/RecurrentPPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1634.30 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1652.31 +/- 5.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1736.78 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1803.64 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2574.77 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3241.53 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2373.76 +/- 3.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1267.57 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1096.65 +/- 1.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1322.50 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1532.13 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1385.07 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1456.01 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1387.01 +/- 7.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1033.40 +/- 6.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-614.60 +/- 11.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=-1317.23 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1222.60 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1369.33 +/- 3.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-428.58 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=-344.26 +/- 6.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-303.03 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=-372.45 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-455.11 +/- 11.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-730.11 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-726.37 +/- 10.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1481.70 +/- 10.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2445.28 +/- 7.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-3146.57 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2866.44 +/- 3.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-3013.97 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2803.04 +/- 6.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-2579.08 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2861.28 +/- 5.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-3139.76 +/- 9.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-3564.84 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-3495.33 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-3136.38 +/- 5.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-3558.19 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3910.91 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-4055.86 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-4130.80 +/- 1.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-3460.88 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-3262.43 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-3349.23 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-3270.66 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-3308.18 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-3554.20 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-3873.66 +/- 31.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-4248.06 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-4093.02 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-4244.28 +/- 18.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-4177.76 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-4254.81 +/- 19.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-3990.09 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-3892.83 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-4208.86 +/- 10.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-4202.14 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-3931.57 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-4237.79 +/- 9.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-4392.30 +/- 2.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-5111.05 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-4899.00 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-4903.78 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-5039.24 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-5178.73 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-5330.40 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-4964.76 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-4690.51 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-4757.89 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-5255.72 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-4833.97 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-4478.72 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-4547.98 +/- 6.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-4621.36 +/- 8.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-4763.44 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-4551.88 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-4691.26 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-4900.10 +/- 4.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-5109.47 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-5043.17 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-4829.63 +/- 5.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-4689.99 +/- 5.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-4128.65 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-4406.84 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-4192.94 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-4753.87 +/- 7.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-4834.45 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-4972.79 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-4900.25 +/- 7.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-4908.02 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-4900.00 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-4683.57 +/- 3.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-4690.15 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-4555.86 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-4554.10 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-4337.67 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-4201.08 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-4055.41 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-3916.28 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-3772.46 +/- 5.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-4126.28 +/- 4.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-4337.23 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-4614.09 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-3980.76 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-4051.97 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-4120.57 +/- 6.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-3897.03 +/- 6.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-3957.61 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-4156.68 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-4087.54 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-1870.71 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-3314.16 +/- 2.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-2340.16 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-2399.24 +/- 7.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-3065.71 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-4707.89 +/- 4.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-4701.44 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-4640.33 +/- 12.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-3167.74 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-3093.87 +/- 2.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-2873.78 +/- 7.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-2345.88 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-2131.44 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-2017.97 +/- 96.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-2949.11 +/- 20.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-3160.57 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-3074.12 +/- 7.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-2990.48 +/- 10.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-3145.27 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-2981.76 +/- 7.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-1863.20 +/- 16.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-2132.61 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-3207.99 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-3196.20 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-2990.28 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-3042.04 +/- 8.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-1801.26 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-1158.93 +/- 137.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-99.97 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=705000, episode_reward=-161.04 +/- 83.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-1661.32 +/- 101.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-333.29 +/- 93.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-456.27 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-433.50 +/- 132.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-479.53 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-2554.82 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-2758.76 +/- 7.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-2759.92 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-3173.60 +/- 6.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-2672.49 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-3020.81 +/- 10.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-2177.90 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-2098.71 +/- 5.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-1679.32 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-1390.71 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-2191.37 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-2265.84 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-1737.20 +/- 7.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-1040.53 +/- 3.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-2150.46 +/- 5.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-2149.23 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-2541.46 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-1146.56 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-528.63 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-1607.06 +/- 18.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-1319.94 +/- 9.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-1693.62 +/- 13.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-2427.34 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-1674.74 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-2123.21 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-3278.99 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-3837.73 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-2971.45 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-3127.82 +/- 12.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-3480.43 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-3479.23 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-4691.50 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-4756.03 +/- 8.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-4955.58 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-5707.46 +/- 7.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-5758.32 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-4986.25 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-5139.26 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-5069.41 +/- 21.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-4563.25 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-4058.23 +/- 8.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-4192.57 +/- 8.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-2938.26 +/- 7.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-2593.50 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-2597.11 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-1938.22 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-1447.25 +/- 3.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-1164.26 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-503.04 +/- 2.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-709.37 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-178.70 +/- 3.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-1144.61 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=524.75 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=-986.44 +/- 7.81\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='RecurrentPPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/ARS/default/\n",
      "Eval num_timesteps=5000, episode_reward=-655725.86 +/- 58.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-6479.00 +/- 134.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-2277.81 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-242.43 +/- 135.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-2855.89 +/- 150.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1738.05 +/- 118.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1512.83 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=2294.30 +/- 80.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=2203.68 +/- 83.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=2030.91 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=2286.13 +/- 62.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=2113.69 +/- 249.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1892.87 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=2098.06 +/- 123.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=2035.59 +/- 104.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=2169.18 +/- 39.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1700.08 +/- 40.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=2091.42 +/- 99.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=2088.59 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1884.11 +/- 551.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1961.27 +/- 106.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1985.38 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=2114.78 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1764.01 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=2107.73 +/- 115.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=2066.83 +/- 122.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=2153.96 +/- 194.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1992.86 +/- 49.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1990.51 +/- 275.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=2023.68 +/- 212.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1991.62 +/- 118.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=2080.86 +/- 213.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=2064.43 +/- 93.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=2153.76 +/- 69.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1995.54 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1967.47 +/- 107.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1689.84 +/- 632.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1919.80 +/- 213.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=2066.78 +/- 83.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1997.35 +/- 106.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=2026.49 +/- 98.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1681.63 +/- 406.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1964.07 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1965.91 +/- 260.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=2006.75 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1956.35 +/- 375.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1902.47 +/- 88.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1777.26 +/- 42.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1921.61 +/- 54.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=2038.95 +/- 79.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1990.96 +/- 214.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1942.62 +/- 129.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1938.61 +/- 37.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1965.70 +/- 77.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1980.66 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1616.05 +/- 449.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1708.00 +/- 329.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1302.84 +/- 876.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1843.33 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1860.84 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1849.37 +/- 118.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1844.47 +/- 168.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1960.95 +/- 227.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=891.61 +/- 842.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1624.03 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1892.75 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1744.61 +/- 109.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1559.40 +/- 518.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1997.82 +/- 82.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1573.61 +/- 609.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1663.28 +/- 397.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1818.67 +/- 265.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1720.63 +/- 839.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1261.80 +/- 939.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1599.90 +/- 861.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1577.81 +/- 792.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1280.78 +/- 537.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1194.37 +/- 867.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1947.39 +/- 83.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1053.90 +/- 1109.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1422.15 +/- 776.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1011.20 +/- 1704.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=919.13 +/- 1087.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=690.13 +/- 1477.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1571.89 +/- 196.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=2004.84 +/- 287.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1620.37 +/- 723.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1659.83 +/- 558.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1325.58 +/- 771.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=2109.30 +/- 96.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1254.58 +/- 538.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1200.54 +/- 787.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=766.73 +/- 615.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1815.59 +/- 60.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1229.70 +/- 642.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1092.53 +/- 682.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.22 +/- 1011.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1823.78 +/- 383.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1421.61 +/- 379.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=881.87 +/- 1224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1427.69 +/- 687.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1891.52 +/- 170.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1862.40 +/- 157.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1571.66 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1378.03 +/- 975.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=769.22 +/- 975.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1681.27 +/- 266.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1866.71 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1974.51 +/- 63.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1777.50 +/- 299.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1425.34 +/- 630.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=836.36 +/- 660.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1410.41 +/- 920.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1304.80 +/- 724.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1323.87 +/- 780.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1950.02 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1751.46 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1794.04 +/- 103.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1345.31 +/- 451.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1710.67 +/- 89.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1698.79 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1496.21 +/- 311.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1777.68 +/- 48.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1845.89 +/- 192.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1593.00 +/- 282.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1474.97 +/- 213.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1660.14 +/- 175.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1907.80 +/- 286.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1652.98 +/- 61.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1842.45 +/- 102.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=2036.13 +/- 102.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1998.44 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1993.81 +/- 73.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1929.45 +/- 91.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1964.35 +/- 119.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=2028.28 +/- 79.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=2062.65 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1920.07 +/- 231.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1967.37 +/- 105.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2128.98 +/- 139.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=2194.64 +/- 51.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=2110.24 +/- 115.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=2151.39 +/- 82.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=2255.17 +/- 83.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=2193.90 +/- 116.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=2031.12 +/- 178.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=2127.29 +/- 169.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=2162.72 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=2176.38 +/- 103.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=2213.38 +/- 73.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=2180.14 +/- 127.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=2232.97 +/- 66.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=2174.36 +/- 120.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=2175.88 +/- 128.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=2114.87 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=2208.02 +/- 120.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=2216.27 +/- 84.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=2139.47 +/- 112.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=2174.62 +/- 181.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=2202.56 +/- 70.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=2198.26 +/- 79.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=2126.81 +/- 202.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=2220.68 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=2206.57 +/- 111.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=2222.79 +/- 58.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=2221.35 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=2269.06 +/- 48.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=2292.92 +/- 58.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=2215.20 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=2274.64 +/- 109.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=2153.46 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=2287.87 +/- 80.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=2263.74 +/- 73.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=2225.86 +/- 82.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=2349.90 +/- 57.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=2160.98 +/- 87.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=2270.19 +/- 71.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=2231.18 +/- 95.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=2242.26 +/- 103.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=2264.88 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=2303.56 +/- 126.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=2195.44 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=2292.48 +/- 156.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=2382.00 +/- 113.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=925000, episode_reward=2318.46 +/- 76.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=2177.38 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=2385.27 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=2327.72 +/- 133.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=2263.87 +/- 82.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=2325.86 +/- 209.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=2266.75 +/- 92.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=2375.73 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=2273.82 +/- 83.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=2307.21 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=2265.13 +/- 122.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=2222.38 +/- 204.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=2226.22 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=2442.54 +/- 79.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=995000, episode_reward=2326.54 +/- 144.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=2379.85 +/- 90.60\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='ARS', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directory ./data/NetworkManagement-v1-100/TQC/default/\n",
      "Eval num_timesteps=5000, episode_reward=-2029087.67 +/- 15.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-2029071.87 +/- 8.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-2029081.64 +/- 9.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-2029081.12 +/- 8.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2029068.53 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-2029078.27 +/- 7.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2029073.02 +/- 8.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-2029087.14 +/- 7.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-2029072.32 +/- 7.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-2029074.13 +/- 8.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-2029070.92 +/- 13.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-2029075.30 +/- 7.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-2029079.75 +/- 8.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-2029080.59 +/- 11.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-2029079.75 +/- 9.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-2029075.95 +/- 11.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-2029077.12 +/- 7.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-2029075.50 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-2029081.38 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-2029079.33 +/- 10.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-2029069.04 +/- 9.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-2029076.10 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-2029074.31 +/- 6.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-2029071.10 +/- 9.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-2029078.12 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-2029083.27 +/- 13.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-2029084.71 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2029075.03 +/- 8.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-2029072.62 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2029081.76 +/- 4.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-2029070.74 +/- 8.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2029074.28 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-2029077.50 +/- 11.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2029081.37 +/- 5.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-2029084.02 +/- 13.19\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_on_env(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNetworkManagement-v1-100\u001b[39;49m\u001b[39m'\u001b[39;49m, algo_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTQC\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m, n_envs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m5e3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algo_name, name, n_envs, timesteps, eval_freq, env_seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[39m=\u001b[39m make_model(algo_name, env, n_steps\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods, batch_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods\u001b[39m*\u001b[39mn_envs)\n\u001b[1;32m     46\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39msave_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, log_path\u001b[39m=\u001b[39msave_path, \n\u001b[1;32m     47\u001b[0m                                 eval_freq\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(eval_freq), deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/tqc/tqc.py:296\u001b[0m, in \u001b[0;36mTQC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39m: SelfTQC,\n\u001b[1;32m    288\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    294\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTQC:\n\u001b[0;32m--> 296\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    297\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    298\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    299\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    300\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    301\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    302\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    303\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:353\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 353\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    355\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    357\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/tqc/tqc.py:187\u001b[0m, in \u001b[0;36mTQC.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, gradient_steps: \u001b[39mint\u001b[39m, batch_size: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    186\u001b[0m     \u001b[39m# Switch to train mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mset_training_mode(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    188\u001b[0m     \u001b[39m# Update optimizers learning rate\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     optimizers \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39moptimizer]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/tqc/policies.py:412\u001b[0m, in \u001b[0;36mTQCPolicy.set_training_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[39mPut the policy in either training or evaluation mode.\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39mThis affects certain modules, such as batch normalisation and dropout.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[39m:param mode: if true, set to training mode, else set to evaluation mode\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mset_training_mode(mode)\n\u001b[0;32m--> 412\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39mset_training_mode(mode)\n\u001b[1;32m    413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m mode\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1259\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1258\u001b[0m     _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m-> 1259\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m _parameters:\n\u001b[1;32m   1260\u001b[0m         \u001b[39mreturn\u001b[39;00m _parameters[name]\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_buffers\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TQC', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 10:44:50) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
