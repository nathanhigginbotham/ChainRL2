{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "from common import make_env\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Loading existing model...\n",
      "Eval num_timesteps=5000, episode_reward=951.73 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=1175.71 +/- 4.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=937.57 +/- 6.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=1004.18 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=905.00 +/- 4.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=981.10 +/- 5.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=844.88 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=737.42 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=672.13 +/- 5.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=664.67 +/- 3.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=752.16 +/- 5.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=654.72 +/- 2.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=616.60 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=448.01 +/- 4.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=571.33 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=304.33 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=369.25 +/- 2.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=475.14 +/- 1.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=518.04 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=627.38 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=827.07 +/- 1.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=421.20 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=761.55 +/- 2.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1018.48 +/- 5.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=585.52 +/- 1.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=799.07 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=850.06 +/- 4.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1018.41 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1024.31 +/- 1.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1013.36 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1045.06 +/- 2.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1056.55 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1233.74 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=974.47 +/- 3.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=487.94 +/- 4.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=424.73 +/- 5.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=555.11 +/- 6.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=567.82 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=386.98 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=769.36 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=621.10 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=612.58 +/- 2.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=571.22 +/- 4.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=705.29 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=683.35 +/- 4.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=795.22 +/- 4.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=995.41 +/- 3.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1106.48 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1054.17 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1208.22 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1147.31 +/- 1.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1121.28 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=956.38 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1073.33 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1225.79 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1151.75 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1206.38 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1058.65 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1283.34 +/- 5.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=300000, episode_reward=1326.42 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=305000, episode_reward=1359.21 +/- 5.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=310000, episode_reward=1366.37 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=315000, episode_reward=1416.33 +/- 3.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=1548.77 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=1547.81 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1360.72 +/- 6.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1436.92 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1525.26 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1515.62 +/- 1.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1399.30 +/- 5.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1256.14 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1404.45 +/- 3.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1346.37 +/- 2.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1208.01 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1195.61 +/- 3.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1417.19 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1298.18 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1470.15 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1489.13 +/- 4.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1373.89 +/- 7.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1441.97 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1375.53 +/- 2.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1450.08 +/- 2.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1464.82 +/- 3.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1407.15 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1318.69 +/- 7.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1141.19 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=949.18 +/- 4.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=785.06 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=395.29 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=560.34 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=609.91 +/- 2.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=550.55 +/- 3.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=707.39 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=759.79 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=791.91 +/- 2.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=647.21 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=608.62 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=793.29 +/- 0.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=549.67 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=314.94 +/- 2.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=321.37 +/- 4.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=323.94 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=473.83 +/- 8.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=730.35 +/- 2.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=741.22 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=929.99 +/- 9.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=850.12 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=701.43 +/- 5.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=612.42 +/- 2.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=664.13 +/- 3.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=478.44 +/- 3.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=918.76 +/- 5.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=760.09 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=626.66 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=507.34 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=681.45 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=704.72 +/- 2.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=494.71 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=592.89 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=492.08 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=713.17 +/- 1.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=335.34 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=406.72 +/- 8.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=814.34 +/- 3.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=768.43 +/- 7.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=532.25 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=277.71 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=454.26 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=499.67 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=634.27 +/- 3.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=570.68 +/- 4.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=473.31 +/- 5.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=832.60 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=867.41 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=718.76 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=494.98 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=460.07 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=883.32 +/- 4.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=476.58 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=423.15 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=710.76 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=871.01 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=853.22 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1083.40 +/- 9.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1064.56 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=461.42 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=467.86 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=268.66 +/- 4.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=911.33 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=659.02 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=479.16 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=510.74 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=831.11 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=746.72 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=496.57 +/- 2.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=511.85 +/- 4.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=620.03 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=741.78 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=489.87 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=187.19 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=415.58 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=614.79 +/- 3.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=406.90 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=477.25 +/- 4.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=673.86 +/- 4.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=670.53 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=298.89 +/- 10.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=843.45 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=605.48 +/- 3.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=819.18 +/- 1.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=447.77 +/- 12.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=882.72 +/- 8.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=880.97 +/- 10.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=861.28 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=985.42 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=945.11 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1002.30 +/- 6.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1079.90 +/- 9.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1019.27 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1100.46 +/- 12.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1137.17 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1218.91 +/- 3.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1093.06 +/- 2.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=895.05 +/- 1.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=905.06 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=571.06 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=695.65 +/- 4.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=476.14 +/- 5.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=454.98 +/- 3.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=712.44 +/- 5.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=480.84 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=650.21 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=412.17 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=570.97 +/- 4.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=787.99 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=721.63 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=576.35 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=531.80 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=578.42 +/- 4.16\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "def train_model_on_env(env_name, algo_name, name, n_envs=1, timesteps=int(1e5), eval_freq=int(5e3), env_seed=0, load_existing=True):\n",
    "    save_path = f'./data/{env_name}/{algo_name}/{name}/'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Using existing directory {save_path}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Creating new directory {save_path}')\n",
    "\n",
    "    def make_subproc_env():    \n",
    "        def _init():\n",
    "            make_env(env_name)\n",
    "        return _init\n",
    "    \n",
    "    if n_envs == 1:\n",
    "        env = make_env(env_name)\n",
    "\n",
    "        if load_existing:\n",
    "            env = Monitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = Monitor(env, save_path, override_existing=True)\n",
    "    else:\n",
    "        env = SubprocVecEnv([make_subproc_env() for _ in range(n_envs)])\n",
    "\n",
    "        if load_existing:\n",
    "            env = VecMonitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = VecMonitor(env, save_path, override_existing=True)\n",
    "\n",
    "    def make_model(algo_name, env, n_steps, batch_size):\n",
    "        model_path = save_path + 'best_model.zip'\n",
    "\n",
    "        if algo_name == 'PPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return PPO.load(model_path, env)\n",
    "            else:\n",
    "                return PPO(PPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'RecurrentPPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                RecurrentPPO.load(model_path, env)\n",
    "            else:\n",
    "                return RecurrentPPO(RecurrentActorCriticPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'A2C':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return A2C.load(model_path, env)\n",
    "            else:\n",
    "                return A2C(A2CPolicy, env, n_steps=n_steps)\n",
    "        \n",
    "        if algo_name == 'ARS':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return ARS.load(model_path, env)\n",
    "            else:\n",
    "                return ARS(ARSPolicy, env)\n",
    "\n",
    "        if algo_name == 'SAC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return SAC.load(model_path, env)\n",
    "            else:\n",
    "                return SAC(SACPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TQC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TQC.load(model_path, env)\n",
    "            else:\n",
    "                return TQC(TQCPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TRPO':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TRPO.load(model_path, env)\n",
    "            else:\n",
    "                return TRPO(TRPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "    model = make_model(algo_name, env, n_steps=env.num_periods, batch_size=env.num_periods*n_envs)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=save_path, verbose=1, log_path=save_path, \n",
    "                                    eval_freq=int(eval_freq), deterministic=True, render=False)\n",
    "\n",
    "    model.learn(total_timesteps=int(timesteps), callback=eval_callback)\n",
    "\n",
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/TRPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1466.54 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1407.51 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1380.62 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1365.15 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-1367.34 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-793.46 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-188.12 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-454.58 +/- 3.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-173.95 +/- 2.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=167.51 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=277.26 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=428.61 +/- 2.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=531.72 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=452.48 +/- 1.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=484.41 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=643.13 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=647.98 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=525.94 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=889.05 +/- 5.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=873.25 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=977.15 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=986.09 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=900.15 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=959.23 +/- 6.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1130.49 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1143.90 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1122.68 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=816.19 +/- 24.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1009.15 +/- 3.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=591.41 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=378.58 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=110.11 +/- 3.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=645.87 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1215.40 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=1373.87 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=935.45 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=772.35 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=645.88 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=427.53 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=638.56 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1046.12 +/- 9.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=955.72 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=889.75 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=512.47 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=670.81 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=861.65 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=591.08 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=577.22 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=650.73 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=343.88 +/- 4.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=865.21 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=282.41 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=856.42 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1523.03 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=271.37 +/- 2.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=669.53 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=769.06 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=502.59 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=900.92 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=695.58 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=673.66 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=812.36 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=32.82 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=498.13 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=452.26 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=340.07 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=629.90 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=469.21 +/- 6.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=743.74 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=485.43 +/- 4.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=677.06 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=260.46 +/- 4.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=725.70 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=713.28 +/- 3.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=715.85 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=658.03 +/- 5.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=278.32 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=842.58 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=725.02 +/- 4.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=768.24 +/- 5.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1025.89 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1017.56 +/- 0.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=483.28 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=759.46 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=279.93 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=636.89 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=411.69 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1423.82 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1933.98 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=2349.22 +/- 7.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=455000, episode_reward=2231.58 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1648.08 +/- 3.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=767.44 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=801.43 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=781.98 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=616.30 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=262.05 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=829.96 +/- 5.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=358.04 +/- 4.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=635.31 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=165.28 +/- 1.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-75.93 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-52.45 +/- 5.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=239.75 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.17 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=73.51 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-414.65 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-362.95 +/- 4.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=44.72 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=401.99 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=755.70 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=897.54 +/- 8.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=896.07 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1013.88 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=942.50 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=256.83 +/- 2.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1316.10 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1166.36 +/- 6.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1391.68 +/- 2.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1373.90 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1026.87 +/- 4.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1098.62 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=888.81 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=666.64 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=438.93 +/- 1.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=437.20 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=474.35 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=566.64 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=639.23 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=415.32 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=641.87 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=562.10 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=557.43 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=288.16 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=286.54 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=524.90 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=478.29 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=211.83 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=425.61 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=319.29 +/- 3.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=243.11 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=568.24 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=128.29 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=246.72 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=38.46 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=32.86 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=37.48 +/- 4.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=22.30 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=39.21 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=183.76 +/- 3.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=266.24 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=235.65 +/- 4.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=43.90 +/- 1.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=558.81 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=590.55 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=643.77 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=813.64 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=979.12 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1075.83 +/- 5.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1080.87 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1059.86 +/- 8.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1093.41 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1172.74 +/- 10.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1204.08 +/- 9.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1242.25 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1234.60 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=800.62 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1189.78 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1052.57 +/- 1.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=930.66 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=540.89 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=988.44 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=930.63 +/- 1.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=839.07 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=713.47 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=817.28 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=669.67 +/- 4.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=648.91 +/- 2.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=955.16 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1067.55 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1079.90 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=869.46 +/- 1.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1026.67 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=797.96 +/- 4.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=733.70 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=530.73 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=879.70 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=637.50 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1062.22 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1148.41 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1334.87 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1534.03 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1484.27 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1494.62 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1458.73 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1619.50 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1706.35 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1642.67 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1668.64 +/- 6.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1609.13 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TRPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1523.62 +/- 4.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1521.26 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1292.16 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1415.39 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1408.21 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1484.25 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1401.90 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1096.19 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1134.17 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-905.83 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1187.99 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1045.14 +/- 1.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-797.44 +/- 1.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-507.18 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-470.74 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-177.33 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=71.78 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-24.12 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=132.01 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=267.62 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=410.23 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=436.50 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=290.61 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=510.28 +/- 3.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=646.54 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=972.33 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=929.72 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=721.92 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=324.63 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=759.06 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=648.35 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=672.73 +/- 2.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=500.33 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=497.71 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=716.17 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=646.17 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=70.45 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=539.16 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=563.71 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=414.14 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=320.99 +/- 1.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=524.77 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=549.86 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=581.23 +/- 3.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=518.25 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=486.43 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=528.24 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=621.63 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=559.82 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=511.57 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=487.11 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=327.44 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=768.25 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=802.01 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=736.80 +/- 1.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=851.61 +/- 2.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=786.26 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=889.21 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=682.65 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=647.63 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=621.33 +/- 3.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=311.20 +/- 2.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=674.11 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=833.60 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=497.28 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=637.74 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=677.05 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=413.18 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=424.77 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=507.36 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=591.16 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=647.70 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=557.29 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=598.59 +/- 4.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=830.00 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=742.52 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=584.90 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=529.74 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=516.88 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=582.70 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=522.81 +/- 7.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=438.39 +/- 2.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=521.55 +/- 1.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=577.26 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=680.27 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=630.15 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=470.67 +/- 2.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=489.37 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=446.07 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=659.82 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=423.34 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=746.86 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=776.05 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=630.62 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=549.38 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=671.48 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.93 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=682.85 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=579.33 +/- 0.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=740.48 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=858.86 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=964.12 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=766.47 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=681.28 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=586.94 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=556.03 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=550.89 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=462.81 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=315.58 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=715.75 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=528.14 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=463.00 +/- 2.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=701.41 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=750.82 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=515.96 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=607.09 +/- 1.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=530.09 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=195.37 +/- 2.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-11.73 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=660.02 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=689.09 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=522.97 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=728.94 +/- 6.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=613.46 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=754.02 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=673.61 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=750.66 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1115.33 +/- 3.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=645000, episode_reward=1172.74 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=1104.72 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=998.91 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=969.90 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1038.23 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=903.60 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=706.41 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=504.29 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=479.27 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=696.12 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=506.95 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=733.60 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=688.56 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=655.01 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=409.48 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=570.45 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=509.39 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=552.82 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=709.72 +/- 4.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=564.81 +/- 4.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=629.25 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=540.45 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=519.98 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=585.71 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=510.68 +/- 9.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=438.97 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=420.57 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=701.62 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=628.14 +/- 9.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=516.98 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=763.92 +/- 2.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=712.94 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=746.26 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=490.38 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=788.73 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=625.38 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=548.14 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=780.35 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=790.74 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=763.38 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=611.96 +/- 1.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=608.05 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=638.73 +/- 2.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=601.23 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=608.05 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=637.89 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=591.63 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=661.25 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=721.13 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=702.32 +/- 6.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=650.57 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=606.79 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=616.37 +/- 4.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=562.08 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=609.37 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=589.32 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=578.78 +/- 0.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=692.01 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=715.81 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=771.07 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=668.83 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=590.97 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=326.50 +/- 6.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=438.73 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=398.04 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=612.61 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=587.64 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=645.84 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=605.58 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=882.75 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=613.40 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=635.22 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/RecurrentPPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1634.30 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1652.31 +/- 5.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1736.78 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1803.64 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2574.77 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3241.53 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2373.76 +/- 3.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1267.57 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1096.65 +/- 1.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1322.50 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1532.13 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1385.07 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1456.01 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1387.01 +/- 7.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1033.40 +/- 6.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-614.60 +/- 11.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=-1317.23 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1222.60 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1369.33 +/- 3.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-428.58 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=-344.26 +/- 6.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-303.03 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=-372.45 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-455.11 +/- 11.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-730.11 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-726.37 +/- 10.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1481.70 +/- 10.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2445.28 +/- 7.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-3146.57 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2866.44 +/- 3.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-3013.97 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2803.04 +/- 6.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-2579.08 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2861.28 +/- 5.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-3139.76 +/- 9.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-3564.84 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-3495.33 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-3136.38 +/- 5.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-3558.19 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3910.91 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-4055.86 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-4130.80 +/- 1.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-3460.88 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-3262.43 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-3349.23 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-3270.66 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-3308.18 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-3554.20 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-3873.66 +/- 31.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-4248.06 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-4093.02 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-4244.28 +/- 18.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-4177.76 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-4254.81 +/- 19.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-3990.09 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-3892.83 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-4208.86 +/- 10.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-4202.14 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-3931.57 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-4237.79 +/- 9.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-4392.30 +/- 2.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-5111.05 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-4899.00 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-4903.78 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-5039.24 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-5178.73 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-5330.40 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-4964.76 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-4690.51 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-4757.89 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-5255.72 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-4833.97 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-4478.72 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-4547.98 +/- 6.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-4621.36 +/- 8.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-4763.44 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-4551.88 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-4691.26 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-4900.10 +/- 4.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-5109.47 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-5043.17 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-4829.63 +/- 5.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-4689.99 +/- 5.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-4128.65 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-4406.84 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-4192.94 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-4753.87 +/- 7.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-4834.45 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-4972.79 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-4900.25 +/- 7.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-4908.02 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-4900.00 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-4683.57 +/- 3.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-4690.15 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-4555.86 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-4554.10 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-4337.67 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-4201.08 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-4055.41 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-3916.28 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-3772.46 +/- 5.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-4126.28 +/- 4.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-4337.23 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-4614.09 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-3980.76 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-4051.97 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-4120.57 +/- 6.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-3897.03 +/- 6.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-3957.61 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-4156.68 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-4087.54 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-1870.71 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-3314.16 +/- 2.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-2340.16 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-2399.24 +/- 7.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-3065.71 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-4707.89 +/- 4.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-4701.44 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-4640.33 +/- 12.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-3167.74 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-3093.87 +/- 2.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-2873.78 +/- 7.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-2345.88 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-2131.44 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-2017.97 +/- 96.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-2949.11 +/- 20.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-3160.57 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-3074.12 +/- 7.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-2990.48 +/- 10.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-3145.27 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-2981.76 +/- 7.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-1863.20 +/- 16.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-2132.61 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-3207.99 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-3196.20 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-2990.28 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-3042.04 +/- 8.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-1801.26 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-1158.93 +/- 137.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-99.97 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=705000, episode_reward=-161.04 +/- 83.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-1661.32 +/- 101.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-333.29 +/- 93.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-456.27 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-433.50 +/- 132.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-479.53 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-2554.82 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-2758.76 +/- 7.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-2759.92 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-3173.60 +/- 6.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-2672.49 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-3020.81 +/- 10.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-2177.90 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-2098.71 +/- 5.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-1679.32 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-1390.71 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-2191.37 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-2265.84 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-1737.20 +/- 7.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-1040.53 +/- 3.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-2150.46 +/- 5.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-2149.23 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-2541.46 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-1146.56 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-528.63 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-1607.06 +/- 18.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-1319.94 +/- 9.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-1693.62 +/- 13.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-2427.34 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-1674.74 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-2123.21 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-3278.99 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-3837.73 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-2971.45 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-3127.82 +/- 12.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-3480.43 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-3479.23 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-4691.50 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-4756.03 +/- 8.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-4955.58 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-5707.46 +/- 7.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-5758.32 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-4986.25 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-5139.26 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-5069.41 +/- 21.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-4563.25 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-4058.23 +/- 8.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-4192.57 +/- 8.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-2938.26 +/- 7.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-2593.50 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-2597.11 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-1938.22 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-1447.25 +/- 3.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-1164.26 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-503.04 +/- 2.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-709.37 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-178.70 +/- 3.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-1144.61 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=524.75 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=-986.44 +/- 7.81\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='RecurrentPPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/ARS/default/\n",
      "Loading existing model...\n",
      "Eval num_timesteps=5000, episode_reward=2405.60 +/- 42.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=2384.58 +/- 84.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=2459.64 +/- 149.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=2378.02 +/- 72.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=2395.85 +/- 69.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=2473.84 +/- 69.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=2445.54 +/- 88.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=2426.24 +/- 123.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=2473.02 +/- 59.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=2413.28 +/- 100.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=2449.80 +/- 71.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=2506.67 +/- 54.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=2494.44 +/- 69.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=2449.98 +/- 115.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=2332.27 +/- 82.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=2420.44 +/- 40.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=2450.27 +/- 51.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=2435.76 +/- 118.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=2421.03 +/- 63.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=2474.89 +/- 96.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=2384.28 +/- 95.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=2482.19 +/- 89.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=2372.47 +/- 180.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=2383.43 +/- 104.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=2345.72 +/- 183.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=2300.57 +/- 156.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=2428.12 +/- 89.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=2272.89 +/- 142.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=2375.55 +/- 205.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=2461.04 +/- 68.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=2319.78 +/- 60.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=2483.04 +/- 79.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=2348.86 +/- 148.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=2428.24 +/- 73.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=2464.53 +/- 71.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=2392.83 +/- 182.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=2470.72 +/- 57.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=2441.21 +/- 59.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=2510.88 +/- 42.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=2401.41 +/- 205.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=2432.87 +/- 80.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=2428.25 +/- 57.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=2324.15 +/- 181.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=2393.74 +/- 114.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=2423.30 +/- 80.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=2568.11 +/- 66.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=235000, episode_reward=2415.94 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=2426.63 +/- 75.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=2473.81 +/- 99.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=2379.15 +/- 51.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=2272.26 +/- 174.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=2451.80 +/- 213.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=2484.62 +/- 92.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=2405.94 +/- 94.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=2307.76 +/- 198.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=2345.24 +/- 83.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=2438.77 +/- 144.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=2449.14 +/- 87.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=2390.37 +/- 181.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=2294.02 +/- 185.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=2353.24 +/- 162.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=2355.39 +/- 39.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=2379.61 +/- 8.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=2425.05 +/- 18.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=2420.90 +/- 26.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=2461.06 +/- 37.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=2400.02 +/- 66.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=2285.29 +/- 129.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=2266.75 +/- 179.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=2357.01 +/- 85.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=2525.22 +/- 111.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1926.87 +/- 1002.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1557.32 +/- 1739.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=2344.47 +/- 174.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=2441.05 +/- 94.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=2444.05 +/- 28.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=2445.17 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=2446.79 +/- 47.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=2345.80 +/- 177.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=2331.95 +/- 186.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=2379.62 +/- 93.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=2414.13 +/- 88.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=2464.02 +/- 70.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=2442.22 +/- 54.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=2458.46 +/- 15.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=2345.45 +/- 149.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=2429.86 +/- 89.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=2478.20 +/- 75.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=2481.48 +/- 35.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=2385.03 +/- 179.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=2421.13 +/- 85.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=2406.24 +/- 144.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=2436.09 +/- 79.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=2256.89 +/- 127.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=2413.87 +/- 179.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=2439.65 +/- 126.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=2339.17 +/- 172.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=2385.53 +/- 40.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=2327.40 +/- 130.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=2450.13 +/- 67.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=2390.06 +/- 187.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=2453.18 +/- 36.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=2360.94 +/- 72.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=2343.65 +/- 88.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=2338.56 +/- 60.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=2392.56 +/- 103.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=2445.80 +/- 108.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=2322.46 +/- 153.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=2446.10 +/- 114.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=2376.93 +/- 108.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=2395.51 +/- 165.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=2336.12 +/- 160.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=2463.13 +/- 75.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=2419.05 +/- 95.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=2388.72 +/- 71.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=2466.63 +/- 82.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=2441.84 +/- 63.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=2474.59 +/- 46.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=2413.12 +/- 57.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=2343.85 +/- 80.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=2414.66 +/- 64.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=2407.26 +/- 84.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=2447.52 +/- 81.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=2470.58 +/- 68.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=2404.31 +/- 40.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=2400.49 +/- 81.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=2374.95 +/- 59.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=2411.01 +/- 98.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=2360.42 +/- 112.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=2379.16 +/- 96.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=2493.94 +/- 89.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=2410.56 +/- 63.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=2382.58 +/- 81.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=2467.64 +/- 56.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=2403.13 +/- 93.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=2366.55 +/- 83.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=2460.41 +/- 47.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=2266.85 +/- 198.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=2348.81 +/- 116.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2364.11 +/- 141.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=2459.55 +/- 48.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=2460.74 +/- 39.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=2416.38 +/- 43.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=2427.08 +/- 28.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=2406.71 +/- 84.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=2399.03 +/- 73.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=2490.99 +/- 64.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=2498.60 +/- 39.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=2374.92 +/- 169.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=2470.44 +/- 113.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=2377.91 +/- 188.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=2439.20 +/- 62.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=2381.19 +/- 50.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=2456.18 +/- 41.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=2318.46 +/- 185.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=2434.45 +/- 76.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=2407.99 +/- 83.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=2320.79 +/- 167.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=2450.21 +/- 88.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=2421.43 +/- 79.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=2428.02 +/- 79.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=2321.31 +/- 211.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=2447.72 +/- 107.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=2408.01 +/- 113.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=2407.59 +/- 82.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=2403.04 +/- 123.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=2457.83 +/- 65.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=2503.45 +/- 55.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=2391.53 +/- 218.12\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='ARS', name='default', n_envs=1, timesteps=4e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/A2C/default/\n",
      "Loading existing model...\n",
      "Eval num_timesteps=5000, episode_reward=2510.43 +/- 90.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=2507.32 +/- 24.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=2461.53 +/- 21.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=2453.59 +/- 12.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=2518.40 +/- 26.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=2272.58 +/- 15.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=2527.84 +/- 30.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=2330.65 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1933.43 +/- 9.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1937.05 +/- 2.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=2231.57 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=2415.25 +/- 23.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=2357.52 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=2369.35 +/- 9.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1944.33 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=2354.86 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=2347.42 +/- 29.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=2153.11 +/- 17.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=2164.85 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=2224.06 +/- 17.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=2294.15 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=2345.77 +/- 27.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=2511.53 +/- 83.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=2479.08 +/- 24.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=2530.80 +/- 39.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=2492.59 +/- 48.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=2522.78 +/- 42.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=2609.43 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=145000, episode_reward=2475.22 +/- 24.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=2465.13 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=2482.03 +/- 66.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=2559.11 +/- 78.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=2545.61 +/- 50.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=2548.91 +/- 83.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=2538.60 +/- 98.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=2573.30 +/- 78.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=2424.60 +/- 27.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=2505.19 +/- 52.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=2553.76 +/- 74.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=2572.97 +/- 11.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=2588.64 +/- 29.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=2507.32 +/- 134.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=2518.26 +/- 76.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=2489.16 +/- 65.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=2536.28 +/- 28.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=2518.76 +/- 21.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=2510.93 +/- 17.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=2488.07 +/- 33.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=2508.81 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=2581.28 +/- 33.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=2551.98 +/- 92.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=2421.82 +/- 19.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=2428.45 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=2425.70 +/- 115.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=2545.90 +/- 83.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=2515.41 +/- 36.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=2448.95 +/- 45.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=2368.11 +/- 136.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=2434.13 +/- 53.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=2412.70 +/- 49.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=2412.68 +/- 51.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=2323.87 +/- 124.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=2341.91 +/- 39.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=2446.73 +/- 173.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=2423.56 +/- 116.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=2502.94 +/- 109.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=2411.53 +/- 84.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=2449.01 +/- 112.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=2544.63 +/- 80.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=2225.81 +/- 71.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=2390.34 +/- 135.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=2440.25 +/- 75.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=2310.49 +/- 94.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=2256.85 +/- 28.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=2196.72 +/- 18.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=2271.42 +/- 48.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=2303.92 +/- 62.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=2267.81 +/- 18.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=2105.01 +/- 19.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=2411.23 +/- 49.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=2141.44 +/- 39.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=2285.90 +/- 61.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=2218.94 +/- 97.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=2302.86 +/- 36.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=2286.81 +/- 53.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=2263.18 +/- 110.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=2316.60 +/- 60.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=2354.50 +/- 62.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=2276.22 +/- 89.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=2348.28 +/- 151.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=2345.19 +/- 140.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=2349.12 +/- 121.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=2291.44 +/- 74.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=2264.58 +/- 53.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=2267.02 +/- 66.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=2252.75 +/- 81.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=2300.94 +/- 39.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=2164.60 +/- 33.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=2126.92 +/- 40.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=2148.62 +/- 33.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1989.04 +/- 6.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1979.82 +/- 37.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1999.88 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1993.17 +/- 11.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1964.25 +/- 39.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1986.19 +/- 20.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1977.34 +/- 37.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1905.80 +/- 21.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1811.37 +/- 48.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1823.98 +/- 28.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1837.75 +/- 17.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=1827.88 +/- 26.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1834.93 +/- 14.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1917.50 +/- 78.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1972.97 +/- 72.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1977.15 +/- 27.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1948.45 +/- 28.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=2033.44 +/- 76.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1861.29 +/- 41.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1970.50 +/- 80.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1908.10 +/- 94.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1895.50 +/- 51.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1886.25 +/- 35.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1932.63 +/- 91.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1909.61 +/- 45.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1917.70 +/- 36.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1938.85 +/- 68.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1975.07 +/- 51.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1932.53 +/- 72.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1919.46 +/- 47.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1972.22 +/- 60.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1916.69 +/- 167.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1997.97 +/- 110.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=2077.00 +/- 52.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1985.62 +/- 119.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=2003.13 +/- 62.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=1829.51 +/- 45.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=2011.06 +/- 73.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1919.02 +/- 169.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=1941.77 +/- 59.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=1835.18 +/- 28.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1833.16 +/- 29.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1789.44 +/- 81.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1806.43 +/- 103.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1790.43 +/- 161.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1817.27 +/- 160.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1795.95 +/- 108.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1792.84 +/- 56.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1815.56 +/- 77.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1778.52 +/- 69.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1842.76 +/- 132.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1803.29 +/- 190.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1865.19 +/- 87.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=2055.53 +/- 21.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=1837.99 +/- 83.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=1884.33 +/- 44.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=1862.61 +/- 47.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=1904.98 +/- 175.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1881.58 +/- 161.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1716.74 +/- 80.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1876.08 +/- 41.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1742.37 +/- 153.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1888.52 +/- 110.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1887.41 +/- 144.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1731.47 +/- 45.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1716.32 +/- 102.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=1780.52 +/- 179.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1862.86 +/- 82.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1927.00 +/- 82.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1799.13 +/- 146.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1752.92 +/- 22.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1802.28 +/- 175.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1835.31 +/- 157.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1856.08 +/- 104.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1740.83 +/- 127.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1724.53 +/- 142.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1833.68 +/- 90.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1828.17 +/- 90.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1661.00 +/- 109.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1786.87 +/- 117.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1738.04 +/- 97.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1825.07 +/- 116.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1874.05 +/- 96.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1906.08 +/- 103.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1911.17 +/- 77.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1967.18 +/- 89.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=2032.66 +/- 78.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1938.73 +/- 70.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1952.07 +/- 123.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1990.84 +/- 82.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1727.79 +/- 206.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1921.15 +/- 107.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1844.33 +/- 96.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1908.06 +/- 125.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1905.48 +/- 177.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1965.21 +/- 82.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1880.65 +/- 198.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1871.42 +/- 82.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1895.44 +/- 97.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1922.09 +/- 46.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=1974.85 +/- 161.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=1966.11 +/- 157.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=1964.98 +/- 72.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=2049.11 +/- 126.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=2110.00 +/- 27.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=1959.72 +/- 124.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=1937.95 +/- 64.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=1988.59 +/- 48.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=1962.39 +/- 109.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=1983.40 +/- 45.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=1995.07 +/- 41.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=1829.58 +/- 13.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=1831.07 +/- 21.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=1830.56 +/- 11.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=1835.19 +/- 6.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=1840.33 +/- 10.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=1943.77 +/- 73.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=1910.65 +/- 71.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=2175.05 +/- 76.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=1948.85 +/- 113.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=1907.07 +/- 9.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=2084.10 +/- 53.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=2014.65 +/- 37.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=1998.85 +/- 48.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=2037.21 +/- 87.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=1940.24 +/- 69.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=2112.15 +/- 15.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=2072.09 +/- 45.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=2040.70 +/- 56.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=1881.55 +/- 49.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=2067.38 +/- 116.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=2007.54 +/- 104.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=2048.43 +/- 67.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=2058.08 +/- 83.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=2070.14 +/- 113.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=2066.61 +/- 43.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=2089.86 +/- 16.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=1897.83 +/- 17.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=1828.50 +/- 16.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=1848.17 +/- 68.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=1901.80 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=1899.83 +/- 16.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=1612.90 +/- 9.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=1546.17 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=1823.64 +/- 14.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=1822.73 +/- 11.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=1815.92 +/- 23.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=1819.94 +/- 8.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=1821.92 +/- 8.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=1823.81 +/- 10.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=1818.58 +/- 13.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=1834.76 +/- 9.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=1803.07 +/- 29.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=1806.04 +/- 27.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=1814.13 +/- 13.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=1808.16 +/- 21.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=1826.01 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=1809.13 +/- 24.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=1730.60 +/- 18.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=1730.59 +/- 36.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=1752.38 +/- 13.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=1731.40 +/- 33.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=1727.32 +/- 29.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=1739.32 +/- 23.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=1530.43 +/- 14.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=1806.51 +/- 28.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=1818.91 +/- 15.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=1810.79 +/- 26.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=1728.73 +/- 22.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=1526.79 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=1525.96 +/- 9.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=1742.10 +/- 14.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=1736.24 +/- 27.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=1817.75 +/- 18.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=1818.40 +/- 15.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=1817.99 +/- 7.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=1823.18 +/- 14.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=1745.06 +/- 19.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=1881.59 +/- 21.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=1893.10 +/- 16.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=1897.81 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1410000, episode_reward=1880.03 +/- 15.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=1806.35 +/- 39.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=1824.11 +/- 14.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=1806.36 +/- 19.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=1888.45 +/- 11.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=1877.25 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=1996.26 +/- 36.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=1872.05 +/- 88.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=1816.03 +/- 23.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=1818.47 +/- 9.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=1820.82 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=1826.32 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=1941.81 +/- 61.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=1960.85 +/- 65.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=1919.60 +/- 123.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=1979.96 +/- 64.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=1923.34 +/- 102.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=1820.92 +/- 13.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=1827.55 +/- 7.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=1990.90 +/- 67.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=2012.08 +/- 37.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=1917.87 +/- 72.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=1998.01 +/- 52.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=1942.93 +/- 75.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=1817.14 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=2018.16 +/- 13.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=1970.01 +/- 49.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=1969.48 +/- 57.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=2035.90 +/- 29.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=1803.66 +/- 38.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=2013.22 +/- 24.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=1719.67 +/- 30.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=1827.35 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=1956.49 +/- 20.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=2057.17 +/- 61.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=2089.12 +/- 35.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=1845.20 +/- 103.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=1810.84 +/- 32.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=1831.48 +/- 11.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1605000, episode_reward=1906.00 +/- 20.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=2164.99 +/- 43.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=2174.53 +/- 143.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=2037.26 +/- 26.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=2045.25 +/- 87.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=1909.69 +/- 121.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=1980.58 +/- 82.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=1966.92 +/- 109.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=1923.72 +/- 128.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=2005.95 +/- 109.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=2123.66 +/- 36.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=2078.93 +/- 69.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=2033.75 +/- 108.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=1976.14 +/- 169.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=2081.78 +/- 41.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=2041.96 +/- 79.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=2007.06 +/- 54.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=2101.38 +/- 39.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=1924.97 +/- 6.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=2073.94 +/- 43.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=2032.92 +/- 109.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=2078.85 +/- 31.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=2008.51 +/- 115.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=1924.65 +/- 7.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=2050.75 +/- 74.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=2064.72 +/- 64.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=2120.54 +/- 33.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=2153.52 +/- 120.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=2194.73 +/- 26.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=2166.73 +/- 41.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=2155.98 +/- 28.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=2225.75 +/- 68.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=2212.12 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=2158.36 +/- 129.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=2180.90 +/- 98.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=2159.08 +/- 34.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=2023.79 +/- 139.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=2126.83 +/- 76.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=2124.72 +/- 89.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=2147.80 +/- 90.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=2197.46 +/- 28.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=2140.61 +/- 14.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=2085.82 +/- 24.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=2065.46 +/- 46.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=2070.77 +/- 169.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=2070.41 +/- 17.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=1997.05 +/- 17.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=2062.73 +/- 26.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=2073.30 +/- 10.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=2039.04 +/- 118.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=2093.24 +/- 115.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=2046.12 +/- 180.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=2230.90 +/- 64.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=1938.89 +/- 85.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=2115.04 +/- 92.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=2115.14 +/- 64.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=2098.51 +/- 128.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=2139.43 +/- 103.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=2123.03 +/- 68.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=2181.06 +/- 39.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=2137.41 +/- 90.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=1918.07 +/- 9.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=2135.26 +/- 85.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=2110.24 +/- 105.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=2140.29 +/- 110.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=1928.30 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=1981.44 +/- 93.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=1827.99 +/- 30.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=2170.61 +/- 37.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=2123.44 +/- 104.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=2215.85 +/- 34.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=2080.36 +/- 98.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=2218.83 +/- 77.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=2092.69 +/- 9.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=2076.11 +/- 20.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=2080.64 +/- 18.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=1855.34 +/- 17.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=1871.31 +/- 4.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=2072.51 +/- 21.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=1861.48 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2005000, episode_reward=1869.64 +/- 5.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2010000, episode_reward=2050.77 +/- 18.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2015000, episode_reward=1868.45 +/- 0.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2020000, episode_reward=1858.00 +/- 8.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2025000, episode_reward=2081.86 +/- 14.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2030000, episode_reward=1730.60 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2035000, episode_reward=1885.97 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=1883.93 +/- 7.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2045000, episode_reward=2182.60 +/- 103.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2050000, episode_reward=2078.37 +/- 13.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2055000, episode_reward=2245.82 +/- 42.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2060000, episode_reward=2006.47 +/- 15.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2065000, episode_reward=2003.78 +/- 20.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2070000, episode_reward=2002.35 +/- 24.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2075000, episode_reward=1986.30 +/- 47.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=2004.94 +/- 20.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2085000, episode_reward=2021.20 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2090000, episode_reward=1939.59 +/- 10.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2095000, episode_reward=1721.39 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2100000, episode_reward=1935.36 +/- 10.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2105000, episode_reward=1997.99 +/- 14.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2110000, episode_reward=1917.74 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2115000, episode_reward=2004.21 +/- 13.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=1995.33 +/- 44.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2125000, episode_reward=1932.64 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2130000, episode_reward=1795.90 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2135000, episode_reward=1919.52 +/- 28.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2140000, episode_reward=1930.32 +/- 14.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2145000, episode_reward=1845.63 +/- 30.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2150000, episode_reward=1865.85 +/- 9.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2155000, episode_reward=1845.31 +/- 40.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=1936.23 +/- 7.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2165000, episode_reward=1933.04 +/- 7.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2170000, episode_reward=1650.55 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2175000, episode_reward=1853.52 +/- 17.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2180000, episode_reward=1849.06 +/- 20.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2185000, episode_reward=1867.83 +/- 8.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2190000, episode_reward=1860.91 +/- 7.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2195000, episode_reward=1925.07 +/- 17.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=1928.92 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2205000, episode_reward=1729.78 +/- 6.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2210000, episode_reward=1871.87 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2215000, episode_reward=1869.00 +/- 20.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2220000, episode_reward=1865.90 +/- 12.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2225000, episode_reward=1830.53 +/- 25.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2230000, episode_reward=1800.91 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2235000, episode_reward=1806.14 +/- 8.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=1861.67 +/- 18.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2245000, episode_reward=1856.05 +/- 15.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2250000, episode_reward=1893.38 +/- 45.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2255000, episode_reward=1946.38 +/- 63.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2260000, episode_reward=2113.20 +/- 59.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2265000, episode_reward=2085.78 +/- 70.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2270000, episode_reward=2073.34 +/- 54.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2275000, episode_reward=2059.03 +/- 64.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=2125.75 +/- 18.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2285000, episode_reward=2092.28 +/- 62.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2290000, episode_reward=2056.61 +/- 85.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2295000, episode_reward=1944.68 +/- 18.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2300000, episode_reward=2083.65 +/- 46.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2305000, episode_reward=2019.31 +/- 29.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2310000, episode_reward=2122.00 +/- 50.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2315000, episode_reward=2157.05 +/- 150.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=1993.13 +/- 109.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2325000, episode_reward=2004.06 +/- 143.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2330000, episode_reward=2027.98 +/- 28.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2335000, episode_reward=2099.35 +/- 108.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2340000, episode_reward=2146.33 +/- 130.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2345000, episode_reward=2095.79 +/- 29.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2350000, episode_reward=2062.25 +/- 53.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2355000, episode_reward=2094.08 +/- 75.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=2024.75 +/- 62.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2365000, episode_reward=1973.24 +/- 30.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2370000, episode_reward=1802.49 +/- 5.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2375000, episode_reward=1807.45 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2380000, episode_reward=2013.92 +/- 15.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2385000, episode_reward=1975.48 +/- 27.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2390000, episode_reward=2000.52 +/- 26.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2395000, episode_reward=2087.32 +/- 130.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=2017.34 +/- 16.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2405000, episode_reward=2080.97 +/- 64.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2410000, episode_reward=1962.73 +/- 81.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2415000, episode_reward=2019.60 +/- 44.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2420000, episode_reward=1990.14 +/- 42.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2425000, episode_reward=2106.64 +/- 31.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2430000, episode_reward=2059.41 +/- 26.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2435000, episode_reward=1858.92 +/- 21.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=1938.26 +/- 16.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2445000, episode_reward=1931.91 +/- 17.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2450000, episode_reward=1924.02 +/- 49.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2455000, episode_reward=1938.24 +/- 22.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2460000, episode_reward=2094.99 +/- 46.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2465000, episode_reward=2072.40 +/- 76.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2470000, episode_reward=2096.98 +/- 51.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2475000, episode_reward=1913.28 +/- 18.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=2098.93 +/- 36.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2485000, episode_reward=1952.39 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2490000, episode_reward=1948.70 +/- 4.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2495000, episode_reward=1932.96 +/- 17.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2500000, episode_reward=1918.53 +/- 23.49\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='A2C', name='default', n_envs=1, timesteps=2.5e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directory ./data/NetworkManagement-v1-100/SAC/default/\n",
      "Eval num_timesteps=5000, episode_reward=-658057.47 +/- 69.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-658020.77 +/- 118.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-657981.66 +/- 90.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-658044.68 +/- 148.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-49331.48 +/- 220.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-12364.71 +/- 4478.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-1371533.47 +/- 3.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1371538.01 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1371535.99 +/- 2.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1371532.97 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1371534.39 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1371534.41 +/- 1.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1371538.61 +/- 4.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1371532.19 +/- 5.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1371536.09 +/- 5.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-1371532.52 +/- 2.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1371533.43 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1371534.29 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1371539.35 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1371538.34 +/- 1.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1371532.76 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1371539.11 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1371530.44 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1371535.13 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1371532.11 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1371535.55 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1371536.96 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1371535.76 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1371536.63 +/- 2.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1371535.01 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-1371536.19 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1371535.53 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-1371538.01 +/- 3.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1371536.13 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1371536.64 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1371539.87 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-1371534.19 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1371538.35 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-1371538.84 +/- 3.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1371538.69 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 11)) of distribution Normal(loc: torch.Size([1, 11]), scale: torch.Size([1, 11])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_on_env(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNetworkManagement-v1-100\u001b[39;49m\u001b[39m'\u001b[39;49m, algo_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSAC\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m, n_envs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m5e3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algo_name, name, n_envs, timesteps, eval_freq, env_seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[39m=\u001b[39m make_model(algo_name, env, n_steps\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods, batch_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods\u001b[39m*\u001b[39mn_envs)\n\u001b[1;32m     46\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39msave_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, log_path\u001b[39m=\u001b[39msave_path, \n\u001b[1;32m     47\u001b[0m                                 eval_freq\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(eval_freq), deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:299\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    290\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[1;32m    291\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    303\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    304\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    305\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    306\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:334\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    331\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    333\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 334\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    336\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    337\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    338\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    339\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    340\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    341\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:564\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mreset_noise(env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    563\u001b[0m \u001b[39m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39;49mnum_envs)\n\u001b[1;32m    566\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    567\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:395\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     unscaled_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_envs)])\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[39m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     unscaled_action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_last_obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    397\u001b[0m \u001b[39m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/policies.py:343\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    344\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    345\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/policies.py:341\u001b[0m, in \u001b[0;36mSACPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(observation, deterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/policies.py:168\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    166\u001b[0m mean_actions, log_std, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action_dist_params(obs)\n\u001b[1;32m    167\u001b[0m \u001b[39m# Note: the action is squashed\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dist\u001b[39m.\u001b[39;49mactions_from_params(mean_actions, log_std, deterministic\u001b[39m=\u001b[39;49mdeterministic, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:190\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.actions_from_params\u001b[0;34m(self, mean_actions, log_std, deterministic)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactions_from_params\u001b[39m(\u001b[39mself\u001b[39m, mean_actions: th\u001b[39m.\u001b[39mTensor, log_std: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    189\u001b[0m     \u001b[39m# Update the proba distribution\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, log_std)\n\u001b[1;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:224\u001b[0m, in \u001b[0;36mSquashedDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mproba_distribution\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[39mself\u001b[39m: SelfSquashedDiagGaussianDistribution, mean_actions: th\u001b[39m.\u001b[39mTensor, log_std: th\u001b[39m.\u001b[39mTensor\n\u001b[1;32m    223\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSquashedDiagGaussianDistribution:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, log_std)\n\u001b[1;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m action_std \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones_like(mean_actions) \u001b[39m*\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/distribution.py:56\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     55\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 56\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 11)) of distribution Normal(loc: torch.Size([1, 11]), scale: torch.Size([1, 11])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='SAC', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
