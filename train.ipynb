{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "from common import make_env\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_env(env_name, algo_name, name, n_envs=1, timesteps=int(1e5), eval_freq=int(5e3), env_seed=0):\n",
    "    save_path = f'./data/{env_name}/{algo_name}/{name}/'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Using existing directory {save_path}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Creating new directory {save_path}')\n",
    "\n",
    "    def make_subproc_env():    \n",
    "        def _init():\n",
    "            make_env(env_name)\n",
    "        return _init\n",
    "    \n",
    "    if n_envs == 1:\n",
    "        env = make_env(env_name)\n",
    "        env = Monitor(env, save_path)\n",
    "    else:\n",
    "        env = SubprocVecEnv([make_subproc_env() for _ in range(n_envs)])\n",
    "        env = VecMonitor(env, save_path)\n",
    "\n",
    "    def make_model(algo_name, env, n_steps, batch_size):\n",
    "        if algo_name == 'PPO':\n",
    "            return PPO(PPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'RecurrentPPO':\n",
    "            return RecurrentPPO(RecurrentActorCriticPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'A2C':\n",
    "            return A2C(A2CPolicy, env, n_steps=n_steps)\n",
    "        \n",
    "        if algo_name == 'ARS':\n",
    "            return ARS(ARSPolicy, env, n_eval_episodes=1, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'SAC':\n",
    "            return SAC(SACPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TQC':\n",
    "            return TQC(TQCPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TRPO':\n",
    "            return TRPO(TRPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "    model = make_model(algo_name, env, n_steps=env.num_periods, batch_size=env.num_periods*n_envs)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=save_path, verbose=1, log_path=save_path, \n",
    "                                    eval_freq=int(eval_freq), deterministic=True, render=False)\n",
    "\n",
    "    model.learn(total_timesteps=int(timesteps), callback=eval_callback)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/TRPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1466.54 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1407.51 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1380.62 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1365.15 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-1367.34 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-793.46 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-188.12 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-454.58 +/- 3.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-173.95 +/- 2.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=167.51 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=277.26 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=428.61 +/- 2.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=531.72 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=452.48 +/- 1.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=484.41 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=643.13 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=647.98 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=525.94 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=889.05 +/- 5.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=873.25 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=977.15 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=986.09 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=900.15 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=959.23 +/- 6.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1130.49 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1143.90 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1122.68 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=816.19 +/- 24.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1009.15 +/- 3.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=591.41 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=378.58 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=110.11 +/- 3.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=645.87 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1215.40 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=1373.87 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=935.45 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=772.35 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=645.88 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=427.53 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=638.56 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1046.12 +/- 9.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=955.72 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=889.75 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=512.47 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=670.81 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=861.65 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=591.08 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=577.22 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=650.73 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=343.88 +/- 4.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=865.21 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=282.41 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=856.42 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1523.03 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=271.37 +/- 2.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=669.53 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=769.06 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=502.59 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=900.92 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=695.58 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=673.66 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=812.36 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=32.82 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=498.13 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=452.26 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=340.07 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=629.90 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=469.21 +/- 6.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=743.74 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=485.43 +/- 4.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=677.06 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=260.46 +/- 4.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=725.70 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=713.28 +/- 3.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=715.85 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=658.03 +/- 5.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=278.32 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=842.58 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=725.02 +/- 4.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=768.24 +/- 5.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1025.89 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1017.56 +/- 0.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=483.28 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=759.46 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=279.93 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=636.89 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=411.69 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1423.82 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1933.98 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=2349.22 +/- 7.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=455000, episode_reward=2231.58 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1648.08 +/- 3.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=767.44 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=801.43 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=781.98 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=616.30 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=262.05 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=829.96 +/- 5.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=358.04 +/- 4.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=635.31 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=165.28 +/- 1.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-75.93 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-52.45 +/- 5.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=239.75 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.17 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=73.51 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-414.65 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-362.95 +/- 4.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=44.72 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=401.99 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=755.70 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=897.54 +/- 8.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=896.07 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1013.88 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=942.50 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=256.83 +/- 2.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1316.10 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1166.36 +/- 6.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1391.68 +/- 2.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1373.90 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1026.87 +/- 4.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1098.62 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=888.81 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=666.64 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=438.93 +/- 1.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=437.20 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=474.35 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=566.64 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=639.23 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=415.32 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=641.87 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=562.10 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=557.43 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=288.16 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=286.54 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=524.90 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=478.29 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=211.83 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=425.61 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=319.29 +/- 3.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=243.11 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=568.24 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=128.29 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=246.72 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=38.46 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=32.86 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=37.48 +/- 4.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=22.30 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=39.21 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=183.76 +/- 3.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=266.24 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=235.65 +/- 4.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=43.90 +/- 1.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=558.81 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=590.55 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=643.77 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=813.64 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=979.12 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1075.83 +/- 5.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1080.87 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1059.86 +/- 8.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1093.41 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1172.74 +/- 10.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1204.08 +/- 9.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1242.25 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1234.60 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=800.62 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1189.78 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1052.57 +/- 1.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=930.66 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=540.89 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=988.44 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=930.63 +/- 1.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=839.07 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=713.47 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=817.28 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=669.67 +/- 4.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=648.91 +/- 2.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=955.16 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1067.55 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1079.90 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=869.46 +/- 1.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1026.67 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=797.96 +/- 4.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=733.70 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=530.73 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=879.70 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=637.50 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1062.22 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1148.41 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1334.87 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1534.03 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1484.27 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1494.62 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1458.73 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1619.50 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1706.35 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1642.67 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1668.64 +/- 6.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1609.13 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TRPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1523.62 +/- 4.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1521.26 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1292.16 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1415.39 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1408.21 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1484.25 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1401.90 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1096.19 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1134.17 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-905.83 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1187.99 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1045.14 +/- 1.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-797.44 +/- 1.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-507.18 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-470.74 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-177.33 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=71.78 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-24.12 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=132.01 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=267.62 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=410.23 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=436.50 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=290.61 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=510.28 +/- 3.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=646.54 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=972.33 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=929.72 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=721.92 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=324.63 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=759.06 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=648.35 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=672.73 +/- 2.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=500.33 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=497.71 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=716.17 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=646.17 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=70.45 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=539.16 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=563.71 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=414.14 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=320.99 +/- 1.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=524.77 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=549.86 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=581.23 +/- 3.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=518.25 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=486.43 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=528.24 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=621.63 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=559.82 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=511.57 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=487.11 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=327.44 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=768.25 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=802.01 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=736.80 +/- 1.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=851.61 +/- 2.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=786.26 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=889.21 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=682.65 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=647.63 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=621.33 +/- 3.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=311.20 +/- 2.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=674.11 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=833.60 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=497.28 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=637.74 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=677.05 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=413.18 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=424.77 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=507.36 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=591.16 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=647.70 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=557.29 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=598.59 +/- 4.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=830.00 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=742.52 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=584.90 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=529.74 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=516.88 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=582.70 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=522.81 +/- 7.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=438.39 +/- 2.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=521.55 +/- 1.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=577.26 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=680.27 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=630.15 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=470.67 +/- 2.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=489.37 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=446.07 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=659.82 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=423.34 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=746.86 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=776.05 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=630.62 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=549.38 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=671.48 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.93 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=682.85 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=579.33 +/- 0.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=740.48 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=858.86 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=964.12 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=766.47 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=681.28 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=586.94 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=556.03 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=550.89 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=462.81 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=315.58 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=715.75 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=528.14 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=463.00 +/- 2.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=701.41 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=750.82 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=515.96 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=607.09 +/- 1.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=530.09 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=195.37 +/- 2.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-11.73 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=660.02 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=689.09 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=522.97 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=728.94 +/- 6.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=613.46 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=754.02 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=673.61 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=750.66 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1115.33 +/- 3.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=645000, episode_reward=1172.74 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=1104.72 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=998.91 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=969.90 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1038.23 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=903.60 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=706.41 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=504.29 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=479.27 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=696.12 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=506.95 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=733.60 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=688.56 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=655.01 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=409.48 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=570.45 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=509.39 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=552.82 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=709.72 +/- 4.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=564.81 +/- 4.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=629.25 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=540.45 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=519.98 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=585.71 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=510.68 +/- 9.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=438.97 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=420.57 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=701.62 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=628.14 +/- 9.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=516.98 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=763.92 +/- 2.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=712.94 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=746.26 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=490.38 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=788.73 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=625.38 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=548.14 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=780.35 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=790.74 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=763.38 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=611.96 +/- 1.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=608.05 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=638.73 +/- 2.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=601.23 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=608.05 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=637.89 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=591.63 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=661.25 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=721.13 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=702.32 +/- 6.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=650.57 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=606.79 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=616.37 +/- 4.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=562.08 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=609.37 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=589.32 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=578.78 +/- 0.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=692.01 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=715.81 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=771.07 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=668.83 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=590.97 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=326.50 +/- 6.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=438.73 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=398.04 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=612.61 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=587.64 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=645.84 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=605.58 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=882.75 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=613.40 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=635.22 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='RecurrentPPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
