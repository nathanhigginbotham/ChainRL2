{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "from common import make_env\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/TRPO/default/\n",
      "Loading existing model...\n",
      "Eval num_timesteps=5000, episode_reward=2550.23 +/- 15.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=2199.96 +/- 7.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=2053.74 +/- 5.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=2547.31 +/- 140.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=1589.08 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1832.96 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=1892.72 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=1582.47 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=1452.99 +/- 2.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=1900.89 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=2197.95 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=2555.06 +/- 72.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=1822.79 +/- 4.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=1601.03 +/- 4.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=1523.71 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=1398.05 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1565.35 +/- 8.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=1089.79 +/- 8.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=1385.72 +/- 3.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1602.42 +/- 17.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1379.73 +/- 5.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1641.65 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=1673.62 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1750.11 +/- 4.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1820.12 +/- 2.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=1871.99 +/- 3.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=1599.65 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1593.60 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=530.60 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=1336.87 +/- 2.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1477.17 +/- 2.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=1379.38 +/- 3.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=1751.85 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1449.48 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1736.09 +/- 3.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1817.80 +/- 6.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1915.37 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1625.15 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=1852.79 +/- 5.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1582.96 +/- 3.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1565.79 +/- 2.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1570.02 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1430.26 +/- 1.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=2603.11 +/- 153.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=225000, episode_reward=2564.81 +/- 7.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1304.32 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=2275.42 +/- 2.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1584.59 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1550.40 +/- 5.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=1087.30 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=928.43 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1283.65 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=967.86 +/- 6.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1415.00 +/- 3.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=648.06 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=595.29 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=285.55 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=521.62 +/- 3.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=291.72 +/- 3.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=141.79 +/- 2.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=161.47 +/- 2.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=179.28 +/- 7.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=178.04 +/- 4.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=178.80 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=203.44 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=175.86 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=147.95 +/- 5.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=380.96 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-527.52 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-985.28 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-864.85 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-531.06 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-738.86 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-289.10 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-68.53 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-983.23 +/- 5.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-986.71 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-972.39 +/- 6.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-983.17 +/- 1.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-790.48 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-767.62 +/- 4.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-771.56 +/- 1.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-772.44 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-772.19 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-541.41 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-540.49 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-768.19 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-773.11 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-803.32 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-818.58 +/- 6.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-821.54 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-844.51 +/- 4.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-982.02 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-144.61 +/- 1.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-768.46 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-765.95 +/- 5.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-645.72 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-563.47 +/- 4.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-549.92 +/- 3.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-80.72 +/- 1.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=90.30 +/- 1.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-66.77 +/- 4.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=179.81 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=202.70 +/- 5.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=202.15 +/- 1.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=194.32 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=180.26 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=124.45 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-294.14 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=151.35 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=208.89 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=196.98 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=149.76 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=131.76 +/- 3.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=121.45 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=164.54 +/- 2.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=90.75 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-68.50 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-356.77 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=122.02 +/- 3.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-67.39 +/- 4.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-754.88 +/- 4.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-528.72 +/- 4.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-8.29 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-663.68 +/- 1.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-93.62 +/- 7.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-44.03 +/- 1.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=149.26 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=91.73 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=167.36 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=180.41 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=191.91 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=207.29 +/- 6.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=179.10 +/- 4.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=181.92 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=97.35 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-70.54 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=93.53 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=216.18 +/- 3.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=180.53 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=210.82 +/- 7.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=885.22 +/- 3.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=515.32 +/- 5.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=169.64 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=597.80 +/- 4.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=151.14 +/- 3.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=220.54 +/- 3.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=72.91 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-3.24 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-224.82 +/- 2.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-452.44 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-603.14 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-300.29 +/- 1.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=79.89 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-144.79 +/- 1.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-601.39 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-447.10 +/- 2.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-375.92 +/- 4.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-856.48 +/- 5.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-856.06 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-632.59 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-721.15 +/- 3.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-894.68 +/- 8.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-1056.99 +/- 9.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-1033.43 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-959.63 +/- 3.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-1084.62 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-797.34 +/- 2.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-796.24 +/- 2.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-633.85 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-783.18 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-287.65 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=161.03 +/- 1.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-643.75 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-1190.58 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-1517.01 +/- 5.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-1805.32 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-1802.01 +/- 1.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-1409.10 +/- 3.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-1170.37 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-1443.77 +/- 4.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-2170.73 +/- 1.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-1991.21 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-1806.78 +/- 3.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-2108.42 +/- 6.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-1527.63 +/- 5.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-1573.21 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-1890.56 +/- 4.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-1904.86 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-1884.65 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-2050.07 +/- 5.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-2094.50 +/- 3.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-1971.66 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-1819.07 +/- 7.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-1227.08 +/- 3.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-890.00 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-1244.67 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-1168.34 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=-1083.37 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=-1299.19 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1005000, episode_reward=-1574.72 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=-1778.27 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=-1912.64 +/- 5.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=-1736.37 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=-1653.55 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=-1457.57 +/- 2.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=-1488.70 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=-1558.89 +/- 6.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=-1469.50 +/- 4.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=-1463.66 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1055000, episode_reward=-1452.87 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1060000, episode_reward=-1569.10 +/- 3.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=-1570.82 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=-1534.21 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=-1455.61 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=-1445.76 +/- 6.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=-1451.50 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=-1569.18 +/- 2.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=-1446.88 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=-1460.13 +/- 5.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=-1234.98 +/- 3.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=-1321.24 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=-1441.38 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=-1033.95 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=-1055.04 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=-732.30 +/- 2.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=-724.67 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=-497.84 +/- 4.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=-302.27 +/- 5.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=-235.09 +/- 4.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=-312.21 +/- 4.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=-374.90 +/- 2.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=-514.86 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=-512.60 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=-605.45 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=-841.86 +/- 3.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=-1102.95 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=-810.72 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=-425.48 +/- 2.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=-683.67 +/- 4.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=-607.36 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=-647.29 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=-83.63 +/- 4.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=688.29 +/- 5.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=460.03 +/- 1.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=844.73 +/- 4.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=1341.67 +/- 6.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=1115.93 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=941.33 +/- 3.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=1130.01 +/- 4.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=702.57 +/- 4.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=1092.38 +/- 2.26\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m\n\u001b[1;32m     84\u001b[0m     eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39msave_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, log_path\u001b[39m=\u001b[39msave_path, \n\u001b[1;32m     85\u001b[0m                                     eval_freq\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(eval_freq), deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m     model\u001b[39m.\u001b[39mlearn(total_timesteps\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(timesteps), callback\u001b[39m=\u001b[39meval_callback)\n\u001b[0;32m---> 89\u001b[0m train_model_on_env(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNetworkManagement-v1-100\u001b[39;49m\u001b[39m'\u001b[39;49m, algo_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mTRPO\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m, n_envs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m4e6\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m5e3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 87\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algo_name, name, n_envs, timesteps, eval_freq, env_seed, load_existing)\u001b[0m\n\u001b[1;32m     82\u001b[0m model \u001b[39m=\u001b[39m make_model(algo_name, env, n_steps\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods, batch_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods\u001b[39m*\u001b[39mn_envs)\n\u001b[1;32m     84\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39msave_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, log_path\u001b[39m=\u001b[39msave_path, \n\u001b[1;32m     85\u001b[0m                                 eval_freq\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(eval_freq), deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 87\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/sb3_contrib/trpo/trpo.py:412\u001b[0m, in \u001b[0;36mTRPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    403\u001b[0m     \u001b[39mself\u001b[39m: SelfTRPO,\n\u001b[1;32m    404\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    409\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfTRPO:\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    413\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    414\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    415\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    416\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    417\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    418\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    419\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:248\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    244\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    246\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 248\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m continue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:175\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n\u001b[1;32m    173\u001b[0m     clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[0;32m--> 175\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[1;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    179\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/or_gym/envs/supply_chain/network_management.py:478\u001b[0m, in \u001b[0;36mNetInvMgmtMasterEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m--> 478\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_STEP(action)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/or_gym/envs/supply_chain/network_management.py:453\u001b[0m, in \u001b[0;36mNetInvMgmtMasterEnv._STEP\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    452\u001b[0m         UP \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 453\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mP\u001b[39m.\u001b[39;49mloc[t,j] \u001b[39m=\u001b[39m a\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mt \u001b[39m*\u001b[39m (SR \u001b[39m-\u001b[39m PC \u001b[39m-\u001b[39m OC \u001b[39m-\u001b[39m HC \u001b[39m-\u001b[39m UP)\n\u001b[1;32m    455\u001b[0m \u001b[39m# update period\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperiod \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[0;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/indexing.py:1688\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     take_split_path \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m can_hold_element(\n\u001b[1;32m   1682\u001b[0m         arr, extract_array(val, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1683\u001b[0m     )\n\u001b[1;32m   1685\u001b[0m \u001b[39m# if we have any multi-indexes that have non-trivial slices\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m# (not null slices) then we must take the split path, xref\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[39m# GH 10360, GH 27841\u001b[39;00m\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(indexer, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(indexer) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49maxes):\n\u001b[1;32m   1689\u001b[0m     \u001b[39mfor\u001b[39;00m i, ax \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(indexer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39maxes):\n\u001b[1;32m   1690\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ax, MultiIndex) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m   1691\u001b[0m             is_integer(i) \u001b[39mor\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(i)\n\u001b[1;32m   1692\u001b[0m         ):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/pandas/core/frame.py:857\u001b[0m, in \u001b[0;36mDataFrame.axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[39mreturn\u001b[39;00m PandasDataFrameXchg(\u001b[39mself\u001b[39m, nan_as_null, allow_copy)\n\u001b[1;32m    855\u001b[0m \u001b[39m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    858\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maxes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    859\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[39m    Return a list representing the axes of the DataFrame.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39m    dtype='object')]\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model_on_env(env_name, algo_name, name, n_envs=1, timesteps=int(1e5), eval_freq=int(5e3), env_seed=0, load_existing=True):\n",
    "    save_path = f'./data/{env_name}/{algo_name}/{name}/'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Using existing directory {save_path}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Creating new directory {save_path}')\n",
    "\n",
    "    def make_subproc_env():    \n",
    "        def _init():\n",
    "            make_env(env_name)\n",
    "        return _init\n",
    "    \n",
    "    if n_envs == 1:\n",
    "        env = make_env(env_name)\n",
    "\n",
    "        if load_existing:\n",
    "            env = Monitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = Monitor(env, save_path, override_existing=True)\n",
    "    else:\n",
    "        env = SubprocVecEnv([make_subproc_env() for _ in range(n_envs)])\n",
    "\n",
    "        if load_existing:\n",
    "            env = VecMonitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = VecMonitor(env, save_path, override_existing=True)\n",
    "\n",
    "    def make_model(algo_name, env, n_steps, batch_size):\n",
    "        model_path = save_path + 'best_model.zip'\n",
    "\n",
    "        if algo_name == 'PPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return PPO.load(model_path, env)\n",
    "            else:\n",
    "                return PPO(PPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'RecurrentPPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                RecurrentPPO.load(model_path, env)\n",
    "            else:\n",
    "                return RecurrentPPO(RecurrentActorCriticPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'A2C':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return A2C.load(model_path, env)\n",
    "            else:\n",
    "                return A2C(A2CPolicy, env, n_steps=n_steps)\n",
    "        \n",
    "        if algo_name == 'ARS':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return ARS.load(model_path, env)\n",
    "            else:\n",
    "                return ARS(ARSPolicy, env)\n",
    "\n",
    "        if algo_name == 'SAC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return SAC.load(model_path, env)\n",
    "            else:\n",
    "                return SAC(SACPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TQC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TQC.load(model_path, env)\n",
    "            else:\n",
    "                return TQC(TQCPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TRPO':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TRPO.load(model_path, env)\n",
    "            else:\n",
    "                return TRPO(TRPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "    model = make_model(algo_name, env, n_steps=env.num_periods, batch_size=env.num_periods*n_envs)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=save_path, verbose=1, log_path=save_path, \n",
    "                                    eval_freq=int(eval_freq), deterministic=True, render=False)\n",
    "\n",
    "    model.learn(total_timesteps=int(timesteps), callback=eval_callback)\n",
    "\n",
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TRPO', name='default', n_envs=1, timesteps=4e6, eval_freq=5e3)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
