{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import or_gym\n",
    "import os\n",
    "\n",
    "from common import make_env\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Loading existing model...\n",
      "Eval num_timesteps=5000, episode_reward=951.73 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=1175.71 +/- 4.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=937.57 +/- 6.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=1004.18 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=905.00 +/- 4.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=981.10 +/- 5.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=844.88 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=737.42 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "def train_model_on_env(env_name, algo_name, name, n_envs=1, timesteps=int(1e5), eval_freq=int(5e3), env_seed=0, load_existing=True):\n",
    "    save_path = f'./data/{env_name}/{algo_name}/{name}/'\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        print(f'Using existing directory {save_path}')\n",
    "    \n",
    "    else:\n",
    "        print(f'Creating new directory {save_path}')\n",
    "\n",
    "    def make_subproc_env():    \n",
    "        def _init():\n",
    "            make_env(env_name)\n",
    "        return _init\n",
    "    \n",
    "    if n_envs == 1:\n",
    "        env = make_env(env_name)\n",
    "\n",
    "        if load_existing:\n",
    "            env = Monitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = Monitor(env, save_path, override_existing=True)\n",
    "    else:\n",
    "        env = SubprocVecEnv([make_subproc_env() for _ in range(n_envs)])\n",
    "\n",
    "        if load_existing:\n",
    "            env = VecMonitor(env, save_path, override_existing=False)\n",
    "        else:\n",
    "            env = VecMonitor(env, save_path, override_existing=True)\n",
    "\n",
    "    def make_model(algo_name, env, n_steps, batch_size):\n",
    "        model_path = save_path + 'best_model.zip'\n",
    "\n",
    "        if algo_name == 'PPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return PPO.load(model_path, env)\n",
    "            else:\n",
    "                return PPO(PPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'RecurrentPPO':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                RecurrentPPO.load(model_path, env)\n",
    "            else:\n",
    "                return RecurrentPPO(RecurrentActorCriticPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'A2C':\n",
    "            if load_existing is True and os.path.exists(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return A2C.load(model_path, env)\n",
    "            else:\n",
    "                return A2C(A2CPolicy, env, n_steps=n_steps)\n",
    "        \n",
    "        if algo_name == 'ARS':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return ARS.load(model_path, env)\n",
    "            else:\n",
    "                return ARS(ARSPolicy, env)\n",
    "\n",
    "        if algo_name == 'SAC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return SAC.load(model_path, env)\n",
    "            else:\n",
    "                return SAC(SACPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TQC':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TQC.load(model_path, env)\n",
    "            else:\n",
    "                return TQC(TQCPolicy, env, batch_size=batch_size)\n",
    "\n",
    "        if algo_name == 'TRPO':\n",
    "            if load_existing is True and os.path.isfile(model_path):\n",
    "                print('Loading existing model...')\n",
    "                return TRPO.load(model_path, env)\n",
    "            else:\n",
    "                return TRPO(TRPOPolicy, env, n_steps=n_steps, batch_size=batch_size)\n",
    "\n",
    "    model = make_model(algo_name, env, n_steps=env.num_periods, batch_size=env.num_periods*n_envs)\n",
    "\n",
    "    eval_callback = EvalCallback(env, best_model_save_path=save_path, verbose=1, log_path=save_path, \n",
    "                                    eval_freq=int(eval_freq), deterministic=True, render=False)\n",
    "\n",
    "    model.learn(total_timesteps=int(timesteps), callback=eval_callback)\n",
    "\n",
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/TRPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1466.54 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1407.51 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1380.62 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1365.15 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-1367.34 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-793.46 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-188.12 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-454.58 +/- 3.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-173.95 +/- 2.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=167.51 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=277.26 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=428.61 +/- 2.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=531.72 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=452.48 +/- 1.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=484.41 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=643.13 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=647.98 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=525.94 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=889.05 +/- 5.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=873.25 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=977.15 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=986.09 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=900.15 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=959.23 +/- 6.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=1130.49 +/- 3.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=1143.90 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=1122.68 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=816.19 +/- 24.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1009.15 +/- 3.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=591.41 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=378.58 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=110.11 +/- 3.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=645.87 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=1215.40 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=175000, episode_reward=1373.87 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=935.45 +/- 6.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=772.35 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=645.88 +/- 8.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=427.53 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=638.56 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=1046.12 +/- 9.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=955.72 +/- 8.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=889.75 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=512.47 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=670.81 +/- 5.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=861.65 +/- 13.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=591.08 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=577.22 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=650.73 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=343.88 +/- 4.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=865.21 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=282.41 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=856.42 +/- 6.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1523.03 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=271.37 +/- 2.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=669.53 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=769.06 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=502.59 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=900.92 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=695.58 +/- 3.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=673.66 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=812.36 +/- 7.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=32.82 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=498.13 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=452.26 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=340.07 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=629.90 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=469.21 +/- 6.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=743.74 +/- 5.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=485.43 +/- 4.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=677.06 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=260.46 +/- 4.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=725.70 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=713.28 +/- 3.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=715.85 +/- 4.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=658.03 +/- 5.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=278.32 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=842.58 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=725.02 +/- 4.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=768.24 +/- 5.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1025.89 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1017.56 +/- 0.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=483.28 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=759.46 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=279.93 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=636.89 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=411.69 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1423.82 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1933.98 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=2349.22 +/- 7.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=455000, episode_reward=2231.58 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1648.08 +/- 3.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=767.44 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=801.43 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=781.98 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=616.30 +/- 5.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=262.05 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=829.96 +/- 5.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=358.04 +/- 4.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=635.31 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=165.28 +/- 1.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-75.93 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-52.45 +/- 5.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=239.75 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1.17 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=73.51 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-414.65 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-362.95 +/- 4.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=44.72 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=401.99 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=755.70 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=897.54 +/- 8.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=896.07 +/- 10.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1013.88 +/- 9.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=942.50 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=256.83 +/- 2.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1316.10 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1166.36 +/- 6.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1391.68 +/- 2.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1373.90 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1026.87 +/- 4.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1098.62 +/- 4.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=888.81 +/- 4.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=666.64 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=438.93 +/- 1.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=437.20 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=474.35 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=566.64 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=639.23 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=415.32 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=641.87 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=562.10 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=557.43 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=288.16 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=286.54 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=524.90 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=478.29 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=211.83 +/- 5.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=425.61 +/- 4.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=319.29 +/- 3.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=243.11 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=568.24 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=128.29 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=246.72 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=38.46 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=32.86 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=37.48 +/- 4.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=22.30 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=39.21 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=183.76 +/- 3.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=266.24 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=235.65 +/- 4.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=43.90 +/- 1.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=558.81 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=590.55 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=643.77 +/- 7.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=813.64 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=979.12 +/- 7.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=1075.83 +/- 5.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=1080.87 +/- 2.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=1059.86 +/- 8.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=1093.41 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=1172.74 +/- 10.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=1204.08 +/- 9.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1242.25 +/- 6.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=1234.60 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=800.62 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1189.78 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1052.57 +/- 1.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=930.66 +/- 6.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=540.89 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=988.44 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=930.63 +/- 1.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=839.07 +/- 2.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=713.47 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=817.28 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=669.67 +/- 4.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=648.91 +/- 2.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=955.16 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1067.55 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1079.90 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=869.46 +/- 1.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1026.67 +/- 2.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=797.96 +/- 4.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=733.70 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=530.73 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=879.70 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=637.50 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1062.22 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1148.41 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=1334.87 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1534.03 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=1484.27 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=1494.62 +/- 7.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1458.73 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=1619.50 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=1706.35 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=1642.67 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=1668.64 +/- 6.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=1609.13 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='TRPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/PPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1523.62 +/- 4.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1521.26 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-1292.16 +/- 4.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1415.39 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-1408.21 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-1484.25 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1401.90 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1096.19 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1134.17 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-905.83 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=55000, episode_reward=-1187.99 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1045.14 +/- 1.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-797.44 +/- 1.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=70000, episode_reward=-507.18 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=75000, episode_reward=-470.74 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-177.33 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=71.78 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=90000, episode_reward=-24.12 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=132.01 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=267.62 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=410.23 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=436.50 +/- 3.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=290.61 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=510.28 +/- 3.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=125000, episode_reward=646.54 +/- 5.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=130000, episode_reward=972.33 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=135000, episode_reward=929.72 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=721.92 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=324.63 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=759.06 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=648.35 +/- 2.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=672.73 +/- 2.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=500.33 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=497.71 +/- 2.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=716.17 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=646.17 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=70.45 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=539.16 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=563.71 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=414.14 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=320.99 +/- 1.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=524.77 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=549.86 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=581.23 +/- 3.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=518.25 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=486.43 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=528.24 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=621.63 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=559.82 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=511.57 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=487.11 +/- 1.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=327.44 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=768.25 +/- 3.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=802.01 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=736.80 +/- 1.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=851.61 +/- 2.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=786.26 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=889.21 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=682.65 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=647.63 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=621.33 +/- 3.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=311.20 +/- 2.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=674.11 +/- 2.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=833.60 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=497.28 +/- 3.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=637.74 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=677.05 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=413.18 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=424.77 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=507.36 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=591.16 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=647.70 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=557.29 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=598.59 +/- 4.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=830.00 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=742.52 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=584.90 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=529.74 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=516.88 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=582.70 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=522.81 +/- 7.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=438.39 +/- 2.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=521.55 +/- 1.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=577.26 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=680.27 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=630.15 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=470.67 +/- 2.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=489.37 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=446.07 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=659.82 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=423.34 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=746.86 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=776.05 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=630.62 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=549.38 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=671.48 +/- 7.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.93 +/- 6.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=682.85 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=579.33 +/- 0.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=740.48 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=858.86 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=964.12 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=766.47 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=681.28 +/- 6.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=586.94 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=556.03 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=550.89 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=462.81 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=315.58 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=715.75 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=528.14 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=463.00 +/- 2.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=701.41 +/- 7.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=750.82 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=515.96 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=607.09 +/- 1.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=530.09 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=195.37 +/- 2.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-11.73 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=660.02 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=689.09 +/- 3.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=522.97 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=728.94 +/- 6.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=613.46 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=754.02 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=673.61 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=750.66 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1115.33 +/- 3.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=645000, episode_reward=1172.74 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=650000, episode_reward=1104.72 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=998.91 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=969.90 +/- 2.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1038.23 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=903.60 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=706.41 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=504.29 +/- 4.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=479.27 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=696.12 +/- 6.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=506.95 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=733.60 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=688.56 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=655.01 +/- 4.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=409.48 +/- 2.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=570.45 +/- 7.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=509.39 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=552.82 +/- 3.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=709.72 +/- 4.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=564.81 +/- 4.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=629.25 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=540.45 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=519.98 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=585.71 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=510.68 +/- 9.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=438.97 +/- 5.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=420.57 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=701.62 +/- 4.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=628.14 +/- 9.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=516.98 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=763.92 +/- 2.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=712.94 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=746.26 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=490.38 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=788.73 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=625.38 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=548.14 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=780.35 +/- 3.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=790.74 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=763.38 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=611.96 +/- 1.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=608.05 +/- 2.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=638.73 +/- 2.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=601.23 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=608.05 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=637.89 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=591.63 +/- 4.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=661.25 +/- 5.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=721.13 +/- 1.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=702.32 +/- 6.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=650.57 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=606.79 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=616.37 +/- 4.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=562.08 +/- 3.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=609.37 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=589.32 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=578.78 +/- 0.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=692.01 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=715.81 +/- 6.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=771.07 +/- 4.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=668.83 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=590.97 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=326.50 +/- 6.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=438.73 +/- 5.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=398.04 +/- 4.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=612.61 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=587.64 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=645.84 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=605.58 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=882.75 +/- 3.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=613.40 +/- 4.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=635.22 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='PPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/RecurrentPPO/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1634.30 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1652.31 +/- 5.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1736.78 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-1803.64 +/- 4.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-2574.77 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-3241.53 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-2373.76 +/- 3.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1267.57 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=-1096.65 +/- 1.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-1322.50 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1532.13 +/- 6.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1385.07 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1456.01 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1387.01 +/- 7.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1033.40 +/- 6.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-614.60 +/- 11.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=85000, episode_reward=-1317.23 +/- 5.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1222.60 +/- 9.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1369.33 +/- 3.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-428.58 +/- 10.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=105000, episode_reward=-344.26 +/- 6.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=110000, episode_reward=-303.03 +/- 20.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=115000, episode_reward=-372.45 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-455.11 +/- 11.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-730.11 +/- 16.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-726.37 +/- 10.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1481.70 +/- 10.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-2445.28 +/- 7.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-3146.57 +/- 6.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-2866.44 +/- 3.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-3013.97 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-2803.04 +/- 6.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-2579.08 +/- 7.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-2861.28 +/- 5.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-3139.76 +/- 9.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-3564.84 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-3495.33 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-3136.38 +/- 5.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-3558.19 +/- 5.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-3910.91 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-4055.86 +/- 7.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-4130.80 +/- 1.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-3460.88 +/- 4.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-3262.43 +/- 12.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-3349.23 +/- 11.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-3270.66 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-3308.18 +/- 17.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-3554.20 +/- 14.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=-3873.66 +/- 31.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-4248.06 +/- 8.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=-4093.02 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-4244.28 +/- 18.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=-4177.76 +/- 9.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=-4254.81 +/- 19.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=-3990.09 +/- 19.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-3892.83 +/- 9.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=-4208.86 +/- 10.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-4202.14 +/- 14.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=-3931.57 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-4237.79 +/- 9.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-4392.30 +/- 2.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-5111.05 +/- 3.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=-4899.00 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-4903.78 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=-5039.24 +/- 6.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=-5178.73 +/- 7.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=-5330.40 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=-4964.76 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=-4690.51 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=-4757.89 +/- 3.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=-5255.72 +/- 8.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=-4833.97 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=-4478.72 +/- 6.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=-4547.98 +/- 6.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=-4621.36 +/- 8.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=-4763.44 +/- 5.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=-4551.88 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=-4691.26 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=-4900.10 +/- 4.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=-5109.47 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=-5043.17 +/- 6.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=-4829.63 +/- 5.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=-4689.99 +/- 5.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=-4128.65 +/- 4.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=-4406.84 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=-4192.94 +/- 5.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=-4753.87 +/- 7.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=-4834.45 +/- 6.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=-4972.79 +/- 3.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=-4900.25 +/- 7.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=-4908.02 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=-4900.00 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=-4683.57 +/- 3.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=-4690.15 +/- 4.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=-4555.86 +/- 5.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=-4554.10 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=-4337.67 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=-4201.08 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=-4055.41 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=-3916.28 +/- 7.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=-3772.46 +/- 5.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=-4126.28 +/- 4.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=-4337.23 +/- 4.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=-4614.09 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=-3980.76 +/- 5.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=-4051.97 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=-4120.57 +/- 6.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=-3897.03 +/- 6.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=-3957.61 +/- 6.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=-4156.68 +/- 8.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=-4087.54 +/- 2.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=-1870.71 +/- 5.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=-3314.16 +/- 2.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=-2340.16 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=-2399.24 +/- 7.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=-3065.71 +/- 11.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=-4707.89 +/- 4.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=-4701.44 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=-4640.33 +/- 12.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=-3167.74 +/- 12.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=-3093.87 +/- 2.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=-2873.78 +/- 7.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=-2345.88 +/- 135.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=-2131.44 +/- 122.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=-2017.97 +/- 96.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=-2949.11 +/- 20.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=-3160.57 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=-3074.12 +/- 7.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=-2990.48 +/- 10.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=-3145.27 +/- 11.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=-2981.76 +/- 7.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=-1863.20 +/- 16.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=-2132.61 +/- 23.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=-3207.99 +/- 8.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=-3196.20 +/- 14.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=-2990.28 +/- 11.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=-3042.04 +/- 8.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=-1801.26 +/- 21.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=-1158.93 +/- 137.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=-99.97 +/- 51.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=705000, episode_reward=-161.04 +/- 83.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=-1661.32 +/- 101.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=-333.29 +/- 93.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=-456.27 +/- 139.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=-433.50 +/- 132.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=-479.53 +/- 29.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=-2554.82 +/- 19.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=-2758.76 +/- 7.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=-2759.92 +/- 10.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=-3173.60 +/- 6.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=-2672.49 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=-3020.81 +/- 10.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=-2177.90 +/- 11.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=-2098.71 +/- 5.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=-1679.32 +/- 8.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=-1390.71 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=-2191.37 +/- 3.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=-2265.84 +/- 5.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=-1737.20 +/- 7.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=-1040.53 +/- 3.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=-2150.46 +/- 5.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=-2149.23 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=-2541.46 +/- 5.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=-1146.56 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=-528.63 +/- 41.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=-1607.06 +/- 18.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=-1319.94 +/- 9.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=-1693.62 +/- 13.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=-2427.34 +/- 9.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=-1674.74 +/- 25.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=-2123.21 +/- 13.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=-3278.99 +/- 11.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=-3837.73 +/- 7.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=-2971.45 +/- 12.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=-3127.82 +/- 12.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=-3480.43 +/- 7.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=-3479.23 +/- 21.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=-4691.50 +/- 17.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=-4756.03 +/- 8.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=-4955.58 +/- 5.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=-5707.46 +/- 7.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=-5758.32 +/- 10.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=-4986.25 +/- 6.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=-5139.26 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=-5069.41 +/- 21.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=-4563.25 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=-4058.23 +/- 8.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=-4192.57 +/- 8.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=-2938.26 +/- 7.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=-2593.50 +/- 6.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=-2597.11 +/- 13.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=-1938.22 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=-1447.25 +/- 3.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=-1164.26 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=-503.04 +/- 2.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=-709.37 +/- 13.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=-178.70 +/- 3.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=-1144.61 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=524.75 +/- 24.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000000, episode_reward=-986.44 +/- 7.81\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='RecurrentPPO', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/ARS/default/\n",
      "Eval num_timesteps=5000, episode_reward=-655725.86 +/- 58.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-6479.00 +/- 134.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-2277.81 +/- 158.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-242.43 +/- 135.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25000, episode_reward=-2855.89 +/- 150.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=1738.05 +/- 118.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=1512.83 +/- 106.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=2294.30 +/- 80.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=45000, episode_reward=2203.68 +/- 83.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=2030.91 +/- 173.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=2286.13 +/- 62.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=2113.69 +/- 249.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=1892.87 +/- 305.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=2098.06 +/- 123.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=2035.59 +/- 104.41\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=2169.18 +/- 39.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=1700.08 +/- 40.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=2091.42 +/- 99.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=2088.59 +/- 20.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=1884.11 +/- 551.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=1961.27 +/- 106.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=1985.38 +/- 274.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=2114.78 +/- 176.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=1764.01 +/- 15.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=2107.73 +/- 115.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=2066.83 +/- 122.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=2153.96 +/- 194.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=1992.86 +/- 49.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=1990.51 +/- 275.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=2023.68 +/- 212.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=1991.62 +/- 118.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=2080.86 +/- 213.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=2064.43 +/- 93.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=2153.76 +/- 69.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=1995.54 +/- 130.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=1967.47 +/- 107.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=1689.84 +/- 632.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=1919.80 +/- 213.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=2066.78 +/- 83.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=1997.35 +/- 106.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=2026.49 +/- 98.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=1681.63 +/- 406.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=1964.07 +/- 205.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=1965.91 +/- 260.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=2006.75 +/- 156.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=1956.35 +/- 375.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=1902.47 +/- 88.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=1777.26 +/- 42.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=245000, episode_reward=1921.61 +/- 54.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=2038.95 +/- 79.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=255000, episode_reward=1990.96 +/- 214.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=1942.62 +/- 129.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=265000, episode_reward=1938.61 +/- 37.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=270000, episode_reward=1965.70 +/- 77.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=275000, episode_reward=1980.66 +/- 187.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=1616.05 +/- 449.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=285000, episode_reward=1708.00 +/- 329.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=1302.84 +/- 876.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=295000, episode_reward=1843.33 +/- 125.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=1860.84 +/- 117.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=1849.37 +/- 118.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=1844.47 +/- 168.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=315000, episode_reward=1960.95 +/- 227.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=891.61 +/- 842.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=325000, episode_reward=1624.03 +/- 234.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=330000, episode_reward=1892.75 +/- 127.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=335000, episode_reward=1744.61 +/- 109.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=340000, episode_reward=1559.40 +/- 518.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=345000, episode_reward=1997.82 +/- 82.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=350000, episode_reward=1573.61 +/- 609.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=355000, episode_reward=1663.28 +/- 397.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=360000, episode_reward=1818.67 +/- 265.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=365000, episode_reward=1720.63 +/- 839.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=1261.80 +/- 939.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=1599.90 +/- 861.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=1577.81 +/- 792.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1280.78 +/- 537.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=390000, episode_reward=1194.37 +/- 867.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=395000, episode_reward=1947.39 +/- 83.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=1053.90 +/- 1109.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1422.15 +/- 776.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=410000, episode_reward=1011.20 +/- 1704.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=919.13 +/- 1087.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=690.13 +/- 1477.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1571.89 +/- 196.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=2004.84 +/- 287.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1620.37 +/- 723.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1659.83 +/- 558.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1325.58 +/- 771.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=450000, episode_reward=2109.30 +/- 96.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1254.58 +/- 538.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1200.54 +/- 787.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=766.73 +/- 615.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1815.59 +/- 60.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=475000, episode_reward=1229.70 +/- 642.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1092.53 +/- 682.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=661.22 +/- 1011.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1823.78 +/- 383.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1421.61 +/- 379.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=881.87 +/- 1224.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1427.69 +/- 687.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1891.52 +/- 170.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1862.40 +/- 157.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1571.66 +/- 254.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1378.03 +/- 975.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=769.22 +/- 975.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1681.27 +/- 266.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1866.71 +/- 197.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1974.51 +/- 63.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1777.50 +/- 299.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1425.34 +/- 630.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=560000, episode_reward=836.36 +/- 660.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=565000, episode_reward=1410.41 +/- 920.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=570000, episode_reward=1304.80 +/- 724.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1323.87 +/- 780.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=580000, episode_reward=1950.02 +/- 152.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=585000, episode_reward=1751.46 +/- 211.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1794.04 +/- 103.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1345.31 +/- 451.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=600000, episode_reward=1710.67 +/- 89.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=605000, episode_reward=1698.79 +/- 17.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1496.21 +/- 311.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1777.68 +/- 48.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1845.89 +/- 192.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1593.00 +/- 282.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1474.97 +/- 213.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1660.14 +/- 175.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1907.80 +/- 286.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1652.98 +/- 61.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1842.45 +/- 102.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=2036.13 +/- 102.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1998.44 +/- 159.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1993.81 +/- 73.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=670000, episode_reward=1929.45 +/- 91.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=675000, episode_reward=1964.35 +/- 119.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=2028.28 +/- 79.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=685000, episode_reward=2062.65 +/- 179.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=690000, episode_reward=1920.07 +/- 231.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=1967.37 +/- 105.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2128.98 +/- 139.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=2194.64 +/- 51.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=2110.24 +/- 115.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=2151.39 +/- 82.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=2255.17 +/- 83.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=2193.90 +/- 116.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=2031.12 +/- 178.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=2127.29 +/- 169.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=2162.72 +/- 202.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=2176.38 +/- 103.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=2213.38 +/- 73.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=2180.14 +/- 127.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=2232.97 +/- 66.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=2174.36 +/- 120.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=2175.88 +/- 128.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=775000, episode_reward=2114.87 +/- 137.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=780000, episode_reward=2208.02 +/- 120.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=785000, episode_reward=2216.27 +/- 84.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=2139.47 +/- 112.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=2174.62 +/- 181.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=2202.56 +/- 70.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=2198.26 +/- 79.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=2126.81 +/- 202.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=2220.68 +/- 133.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=2206.57 +/- 111.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=2222.79 +/- 58.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=2221.35 +/- 125.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=2269.06 +/- 48.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=2292.92 +/- 58.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=2215.20 +/- 203.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=2274.64 +/- 109.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=2153.46 +/- 142.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=2287.87 +/- 80.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=2263.74 +/- 73.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=2225.86 +/- 82.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=2349.90 +/- 57.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=2160.98 +/- 87.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=2270.19 +/- 71.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=2231.18 +/- 95.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=2242.26 +/- 103.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=2264.88 +/- 86.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=2303.56 +/- 126.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=2195.44 +/- 185.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=2292.48 +/- 156.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=2382.00 +/- 113.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=925000, episode_reward=2318.46 +/- 76.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=2177.38 +/- 235.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=2385.27 +/- 116.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=2327.72 +/- 133.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=2263.87 +/- 82.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=2325.86 +/- 209.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=2266.75 +/- 92.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=2375.73 +/- 136.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=2273.82 +/- 83.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=2307.21 +/- 41.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=2265.13 +/- 122.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=2222.38 +/- 204.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=2226.22 +/- 143.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=2442.54 +/- 79.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=995000, episode_reward=2326.54 +/- 144.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=2379.85 +/- 90.60\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='ARS', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing directory ./data/NetworkManagement-v1-100/A2C/default/\n",
      "Eval num_timesteps=5000, episode_reward=-1521.55 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-1673.25 +/- 2.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-1445.37 +/- 5.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1506.61 +/- 4.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-929.78 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1449.77 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-1481.48 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1423.92 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1193.68 +/- 1.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1247.52 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1202.48 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-906.83 +/- 2.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=65000, episode_reward=-1196.96 +/- 2.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1425.27 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1117.55 +/- 4.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-1147.35 +/- 4.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1077.21 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1188.05 +/- 1.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1262.02 +/- 3.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1353.96 +/- 2.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1286.46 +/- 6.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1274.72 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1229.22 +/- 6.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1134.30 +/- 3.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1237.15 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1132.39 +/- 2.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1113.79 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1155.84 +/- 3.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1158.85 +/- 4.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1237.31 +/- 5.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-1024.00 +/- 2.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1043.22 +/- 3.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-917.65 +/- 2.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1023.81 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1234.49 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1246.36 +/- 3.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-1303.34 +/- 6.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1302.86 +/- 2.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-1231.72 +/- 5.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1193.65 +/- 3.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=205000, episode_reward=-1224.79 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=210000, episode_reward=-1346.95 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=215000, episode_reward=-1261.23 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=220000, episode_reward=-1091.15 +/- 1.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=225000, episode_reward=-1156.81 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=230000, episode_reward=-1107.49 +/- 1.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=235000, episode_reward=-1131.72 +/- 1.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=240000, episode_reward=-835.02 +/- 4.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=245000, episode_reward=-904.77 +/- 4.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=250000, episode_reward=-770.34 +/- 2.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=255000, episode_reward=-772.35 +/- 4.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=260000, episode_reward=-702.00 +/- 2.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=265000, episode_reward=-674.09 +/- 6.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270000, episode_reward=-643.00 +/- 3.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=275000, episode_reward=-340.18 +/- 4.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=-338.42 +/- 2.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=285000, episode_reward=-336.59 +/- 2.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=290000, episode_reward=-333.97 +/- 3.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=295000, episode_reward=-340.69 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-339.14 +/- 3.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=305000, episode_reward=-115.83 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=310000, episode_reward=-110.61 +/- 6.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=315000, episode_reward=123.09 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=185.31 +/- 4.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=325000, episode_reward=417.34 +/- 4.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=330000, episode_reward=555.67 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=335000, episode_reward=643.27 +/- 2.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=340000, episode_reward=855.82 +/- 4.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=345000, episode_reward=867.05 +/- 2.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=350000, episode_reward=884.43 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=355000, episode_reward=1105.45 +/- 4.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=1112.54 +/- 4.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=365000, episode_reward=943.82 +/- 6.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=370000, episode_reward=788.43 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=375000, episode_reward=786.50 +/- 2.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=380000, episode_reward=870.32 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=385000, episode_reward=1164.05 +/- 2.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=1187.43 +/- 1.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=395000, episode_reward=1011.44 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=400000, episode_reward=992.50 +/- 2.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=405000, episode_reward=1264.29 +/- 1.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=410000, episode_reward=1248.05 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=415000, episode_reward=1168.14 +/- 3.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=420000, episode_reward=1228.99 +/- 2.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=425000, episode_reward=1189.95 +/- 1.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=430000, episode_reward=1124.73 +/- 4.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=435000, episode_reward=1159.59 +/- 2.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=440000, episode_reward=1002.34 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=445000, episode_reward=1304.60 +/- 4.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=1186.40 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=455000, episode_reward=1186.71 +/- 4.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=460000, episode_reward=1294.98 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=465000, episode_reward=1184.83 +/- 6.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=470000, episode_reward=1462.35 +/- 4.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=475000, episode_reward=1362.72 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=480000, episode_reward=1373.00 +/- 5.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=485000, episode_reward=1371.40 +/- 2.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=490000, episode_reward=1335.85 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=495000, episode_reward=1189.68 +/- 3.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=500000, episode_reward=1371.16 +/- 2.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=505000, episode_reward=1366.88 +/- 2.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=510000, episode_reward=1374.65 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=515000, episode_reward=1374.39 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=520000, episode_reward=1189.27 +/- 3.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=525000, episode_reward=1222.57 +/- 4.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=530000, episode_reward=1348.99 +/- 3.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=535000, episode_reward=1347.17 +/- 4.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=540000, episode_reward=1384.29 +/- 2.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=545000, episode_reward=1404.59 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=550000, episode_reward=1402.84 +/- 3.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=555000, episode_reward=1499.46 +/- 5.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=1612.99 +/- 4.77\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=565000, episode_reward=1613.06 +/- 1.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=570000, episode_reward=1610.91 +/- 2.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=575000, episode_reward=1614.37 +/- 5.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=580000, episode_reward=1633.82 +/- 1.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=585000, episode_reward=1633.23 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=590000, episode_reward=1607.29 +/- 2.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=595000, episode_reward=1685.02 +/- 1.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=1819.77 +/- 3.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=605000, episode_reward=1637.02 +/- 3.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=610000, episode_reward=1662.74 +/- 2.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=615000, episode_reward=1686.78 +/- 1.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=620000, episode_reward=1658.52 +/- 1.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=625000, episode_reward=1651.96 +/- 2.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=630000, episode_reward=1658.57 +/- 3.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=635000, episode_reward=1637.61 +/- 7.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=640000, episode_reward=1615.00 +/- 1.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=645000, episode_reward=1640.89 +/- 4.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=650000, episode_reward=1639.30 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=655000, episode_reward=1676.60 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=660000, episode_reward=1676.74 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=665000, episode_reward=1919.49 +/- 5.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=670000, episode_reward=1921.90 +/- 2.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=675000, episode_reward=1792.83 +/- 3.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=680000, episode_reward=2036.52 +/- 2.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=685000, episode_reward=2089.81 +/- 1.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=690000, episode_reward=1946.39 +/- 5.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=695000, episode_reward=2077.66 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=700000, episode_reward=2063.91 +/- 4.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=705000, episode_reward=2042.30 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=710000, episode_reward=1918.80 +/- 2.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=715000, episode_reward=1804.94 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=720000, episode_reward=1804.98 +/- 0.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=725000, episode_reward=1920.02 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=730000, episode_reward=1922.26 +/- 6.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=735000, episode_reward=1923.47 +/- 2.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=740000, episode_reward=1927.58 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=745000, episode_reward=1841.68 +/- 1.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=750000, episode_reward=1795.62 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=755000, episode_reward=1949.81 +/- 5.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=760000, episode_reward=1944.21 +/- 4.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=765000, episode_reward=1962.46 +/- 2.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=770000, episode_reward=2090.93 +/- 6.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=775000, episode_reward=2101.32 +/- 3.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=780000, episode_reward=2116.03 +/- 16.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=785000, episode_reward=1976.92 +/- 6.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=790000, episode_reward=2072.30 +/- 5.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=795000, episode_reward=2072.13 +/- 5.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=800000, episode_reward=2070.72 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=805000, episode_reward=2099.32 +/- 4.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=810000, episode_reward=2073.93 +/- 1.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=815000, episode_reward=2088.83 +/- 7.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=820000, episode_reward=2086.34 +/- 6.68\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=825000, episode_reward=1978.12 +/- 5.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=830000, episode_reward=2083.87 +/- 2.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=835000, episode_reward=2087.43 +/- 7.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=840000, episode_reward=1976.95 +/- 4.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=845000, episode_reward=1956.29 +/- 4.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=850000, episode_reward=1818.74 +/- 3.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=855000, episode_reward=1961.47 +/- 1.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=860000, episode_reward=1829.43 +/- 3.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=865000, episode_reward=1834.56 +/- 2.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=870000, episode_reward=1691.11 +/- 8.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=875000, episode_reward=1693.82 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=880000, episode_reward=1565.39 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=885000, episode_reward=1561.71 +/- 6.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=890000, episode_reward=1688.06 +/- 2.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=895000, episode_reward=1695.29 +/- 2.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=900000, episode_reward=1692.28 +/- 1.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=905000, episode_reward=1831.40 +/- 5.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=910000, episode_reward=1971.09 +/- 2.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=915000, episode_reward=1836.95 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=920000, episode_reward=1954.92 +/- 4.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=925000, episode_reward=1957.16 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=930000, episode_reward=1947.01 +/- 5.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=935000, episode_reward=1940.09 +/- 6.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=940000, episode_reward=1823.66 +/- 5.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=945000, episode_reward=1921.57 +/- 4.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=950000, episode_reward=1942.50 +/- 5.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=955000, episode_reward=2076.36 +/- 5.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=960000, episode_reward=1955.88 +/- 2.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=965000, episode_reward=2078.80 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=970000, episode_reward=2071.32 +/- 11.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=975000, episode_reward=1928.93 +/- 39.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=980000, episode_reward=2053.92 +/- 15.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=985000, episode_reward=2052.98 +/- 43.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=990000, episode_reward=2051.33 +/- 18.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=995000, episode_reward=2064.45 +/- 18.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1000000, episode_reward=2327.47 +/- 30.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1005000, episode_reward=2297.35 +/- 27.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1010000, episode_reward=2323.02 +/- 34.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1015000, episode_reward=2298.71 +/- 34.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1020000, episode_reward=2321.82 +/- 20.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1025000, episode_reward=2207.11 +/- 4.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1030000, episode_reward=2201.80 +/- 9.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1035000, episode_reward=2091.96 +/- 12.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1040000, episode_reward=2205.26 +/- 3.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1045000, episode_reward=2220.04 +/- 5.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1050000, episode_reward=2328.99 +/- 2.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1055000, episode_reward=2341.72 +/- 12.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1060000, episode_reward=2231.38 +/- 11.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1065000, episode_reward=2213.95 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1070000, episode_reward=2202.43 +/- 21.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1075000, episode_reward=2237.51 +/- 10.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1080000, episode_reward=2082.52 +/- 9.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1085000, episode_reward=2218.52 +/- 8.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1090000, episode_reward=2203.32 +/- 10.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1095000, episode_reward=2208.91 +/- 1.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1100000, episode_reward=2190.80 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1105000, episode_reward=2193.98 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1110000, episode_reward=2070.97 +/- 4.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1115000, episode_reward=2056.21 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1120000, episode_reward=2071.61 +/- 4.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1125000, episode_reward=1836.60 +/- 5.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1130000, episode_reward=1837.65 +/- 3.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1135000, episode_reward=1833.11 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1140000, episode_reward=1838.95 +/- 1.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1145000, episode_reward=1911.06 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1150000, episode_reward=1914.43 +/- 4.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1155000, episode_reward=2033.50 +/- 3.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1160000, episode_reward=2116.55 +/- 2.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1165000, episode_reward=1992.17 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1170000, episode_reward=2114.46 +/- 2.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1175000, episode_reward=2046.94 +/- 1.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1180000, episode_reward=2148.23 +/- 5.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1185000, episode_reward=2025.63 +/- 4.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1190000, episode_reward=2085.79 +/- 3.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1195000, episode_reward=1836.96 +/- 4.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1200000, episode_reward=2089.11 +/- 6.76\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1205000, episode_reward=2045.19 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1210000, episode_reward=2045.00 +/- 2.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1215000, episode_reward=2019.17 +/- 5.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1220000, episode_reward=2196.12 +/- 3.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1225000, episode_reward=2314.78 +/- 1.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1230000, episode_reward=2314.91 +/- 3.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1235000, episode_reward=2191.78 +/- 9.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1240000, episode_reward=2194.52 +/- 2.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1245000, episode_reward=2188.22 +/- 1.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1250000, episode_reward=2086.37 +/- 1.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1255000, episode_reward=2195.86 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1260000, episode_reward=2174.43 +/- 2.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1265000, episode_reward=2078.52 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1270000, episode_reward=2077.32 +/- 11.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1275000, episode_reward=1957.67 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1280000, episode_reward=1903.94 +/- 3.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1285000, episode_reward=1906.50 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1290000, episode_reward=1904.44 +/- 3.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1295000, episode_reward=1904.18 +/- 3.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1300000, episode_reward=1943.13 +/- 5.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1305000, episode_reward=1899.43 +/- 1.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1310000, episode_reward=1868.01 +/- 5.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1315000, episode_reward=1895.72 +/- 4.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1320000, episode_reward=1897.04 +/- 4.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1325000, episode_reward=1831.30 +/- 2.83\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1330000, episode_reward=2015.62 +/- 4.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1335000, episode_reward=2004.93 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1340000, episode_reward=2017.22 +/- 3.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1345000, episode_reward=2080.93 +/- 4.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1350000, episode_reward=2082.36 +/- 5.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1355000, episode_reward=1974.95 +/- 3.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1360000, episode_reward=2091.94 +/- 4.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1365000, episode_reward=2089.63 +/- 4.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1370000, episode_reward=2315.90 +/- 3.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1375000, episode_reward=2313.90 +/- 5.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1380000, episode_reward=2312.50 +/- 2.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1385000, episode_reward=2245.14 +/- 1.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1390000, episode_reward=2311.67 +/- 7.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1395000, episode_reward=2329.16 +/- 11.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1400000, episode_reward=2332.79 +/- 10.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1405000, episode_reward=2453.13 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1410000, episode_reward=2327.85 +/- 8.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1415000, episode_reward=2101.01 +/- 3.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1420000, episode_reward=2229.51 +/- 3.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1425000, episode_reward=2430.63 +/- 24.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1430000, episode_reward=2227.98 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1435000, episode_reward=2326.69 +/- 2.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1440000, episode_reward=2220.92 +/- 4.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1445000, episode_reward=2216.88 +/- 6.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1450000, episode_reward=2215.23 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1455000, episode_reward=2315.85 +/- 7.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1460000, episode_reward=2252.19 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1465000, episode_reward=2030.90 +/- 3.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1470000, episode_reward=2028.03 +/- 5.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1475000, episode_reward=2166.99 +/- 18.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1480000, episode_reward=2245.40 +/- 7.23\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1485000, episode_reward=2188.97 +/- 5.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1490000, episode_reward=2116.51 +/- 3.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1495000, episode_reward=2107.83 +/- 9.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1500000, episode_reward=2198.34 +/- 6.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1505000, episode_reward=2127.75 +/- 5.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1510000, episode_reward=2163.31 +/- 8.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1515000, episode_reward=2112.01 +/- 3.92\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1520000, episode_reward=2179.01 +/- 7.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1525000, episode_reward=1974.98 +/- 3.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1530000, episode_reward=2292.63 +/- 9.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1535000, episode_reward=2068.44 +/- 7.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1540000, episode_reward=2183.47 +/- 6.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1545000, episode_reward=2025.60 +/- 3.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1550000, episode_reward=2181.87 +/- 5.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1555000, episode_reward=2354.13 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1560000, episode_reward=2361.95 +/- 4.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1565000, episode_reward=2361.97 +/- 20.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1570000, episode_reward=2362.03 +/- 3.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1575000, episode_reward=2246.18 +/- 5.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1580000, episode_reward=2391.85 +/- 13.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1585000, episode_reward=2203.13 +/- 4.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1590000, episode_reward=2436.48 +/- 28.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1595000, episode_reward=2440.31 +/- 29.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1600000, episode_reward=2570.43 +/- 37.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1605000, episode_reward=2452.74 +/- 24.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1610000, episode_reward=2383.66 +/- 6.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1615000, episode_reward=2487.80 +/- 5.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1620000, episode_reward=2443.03 +/- 15.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1625000, episode_reward=2432.92 +/- 19.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1630000, episode_reward=2275.39 +/- 17.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1635000, episode_reward=2151.44 +/- 3.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1640000, episode_reward=2153.31 +/- 2.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1645000, episode_reward=2104.88 +/- 3.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1650000, episode_reward=2124.39 +/- 4.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1655000, episode_reward=2102.45 +/- 4.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1660000, episode_reward=2034.62 +/- 2.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1665000, episode_reward=2147.06 +/- 6.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1670000, episode_reward=2174.47 +/- 4.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1675000, episode_reward=2245.99 +/- 2.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1680000, episode_reward=2398.82 +/- 12.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1685000, episode_reward=2387.05 +/- 19.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1690000, episode_reward=2398.14 +/- 1.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1695000, episode_reward=2172.91 +/- 2.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1700000, episode_reward=1950.37 +/- 7.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1705000, episode_reward=1841.56 +/- 3.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1710000, episode_reward=2324.22 +/- 6.21\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1715000, episode_reward=1953.64 +/- 4.95\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1720000, episode_reward=1956.04 +/- 5.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1725000, episode_reward=1997.25 +/- 6.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1730000, episode_reward=2323.77 +/- 3.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1735000, episode_reward=2143.95 +/- 3.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1740000, episode_reward=2147.88 +/- 5.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1745000, episode_reward=2232.40 +/- 1.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1750000, episode_reward=2231.44 +/- 5.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1755000, episode_reward=2233.90 +/- 3.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1760000, episode_reward=2072.33 +/- 6.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1765000, episode_reward=1914.95 +/- 5.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1770000, episode_reward=1841.52 +/- 3.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1775000, episode_reward=1850.26 +/- 4.90\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1780000, episode_reward=1841.13 +/- 5.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1785000, episode_reward=1995.77 +/- 12.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1790000, episode_reward=1883.20 +/- 10.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1795000, episode_reward=1962.64 +/- 7.78\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1800000, episode_reward=1982.78 +/- 7.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1805000, episode_reward=1982.31 +/- 2.69\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1810000, episode_reward=1958.92 +/- 4.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1815000, episode_reward=1977.74 +/- 11.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1820000, episode_reward=1987.86 +/- 2.03\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1825000, episode_reward=1919.40 +/- 5.26\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1830000, episode_reward=2135.17 +/- 6.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1835000, episode_reward=2371.51 +/- 45.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1840000, episode_reward=2068.40 +/- 5.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1845000, episode_reward=2353.34 +/- 47.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1850000, episode_reward=2375.64 +/- 30.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1855000, episode_reward=2337.28 +/- 16.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1860000, episode_reward=2370.17 +/- 76.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1865000, episode_reward=2424.07 +/- 4.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1870000, episode_reward=2391.44 +/- 46.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1875000, episode_reward=2194.53 +/- 14.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1880000, episode_reward=2181.26 +/- 9.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1885000, episode_reward=2195.27 +/- 13.56\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1890000, episode_reward=2306.62 +/- 5.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1895000, episode_reward=2043.25 +/- 5.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1900000, episode_reward=2193.38 +/- 4.27\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1905000, episode_reward=2183.00 +/- 6.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1910000, episode_reward=2305.13 +/- 11.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1915000, episode_reward=2302.50 +/- 1.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1920000, episode_reward=2335.21 +/- 5.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1925000, episode_reward=2413.06 +/- 22.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1930000, episode_reward=2208.76 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1935000, episode_reward=2204.09 +/- 6.46\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1940000, episode_reward=2464.16 +/- 27.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1945000, episode_reward=2303.81 +/- 3.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1950000, episode_reward=2311.54 +/- 8.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1955000, episode_reward=2395.74 +/- 4.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1960000, episode_reward=2485.50 +/- 40.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1965000, episode_reward=2343.66 +/- 11.06\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1970000, episode_reward=2169.15 +/- 3.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1975000, episode_reward=2352.94 +/- 8.70\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1980000, episode_reward=2499.58 +/- 16.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1985000, episode_reward=2316.03 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1990000, episode_reward=2317.49 +/- 5.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=1995000, episode_reward=2446.12 +/- 19.18\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2000000, episode_reward=2326.26 +/- 6.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2005000, episode_reward=2390.50 +/- 6.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2010000, episode_reward=2377.45 +/- 29.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2015000, episode_reward=2390.29 +/- 4.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2020000, episode_reward=2495.08 +/- 16.34\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2025000, episode_reward=2188.34 +/- 4.12\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2030000, episode_reward=2191.54 +/- 4.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2035000, episode_reward=2320.56 +/- 2.49\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2040000, episode_reward=2320.71 +/- 2.43\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2045000, episode_reward=2353.49 +/- 40.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2050000, episode_reward=2524.47 +/- 48.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2055000, episode_reward=2364.10 +/- 14.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2060000, episode_reward=2511.58 +/- 55.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2065000, episode_reward=2250.91 +/- 13.15\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2070000, episode_reward=2355.60 +/- 21.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2075000, episode_reward=2443.85 +/- 118.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2080000, episode_reward=2452.10 +/- 39.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2085000, episode_reward=2595.32 +/- 37.96\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2090000, episode_reward=2530.97 +/- 56.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2095000, episode_reward=2365.73 +/- 11.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2100000, episode_reward=2473.13 +/- 9.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2105000, episode_reward=2465.76 +/- 14.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2110000, episode_reward=2457.18 +/- 40.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2115000, episode_reward=2498.81 +/- 9.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2120000, episode_reward=2558.90 +/- 49.36\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2125000, episode_reward=2436.75 +/- 7.52\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2130000, episode_reward=2602.79 +/- 29.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2135000, episode_reward=2345.03 +/- 27.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2140000, episode_reward=2355.55 +/- 14.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2145000, episode_reward=2349.74 +/- 30.82\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2150000, episode_reward=2359.87 +/- 8.88\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2155000, episode_reward=2533.85 +/- 101.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2160000, episode_reward=2361.42 +/- 7.93\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2165000, episode_reward=2428.39 +/- 8.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2170000, episode_reward=2341.48 +/- 12.40\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2175000, episode_reward=2379.75 +/- 16.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2180000, episode_reward=2374.35 +/- 20.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2185000, episode_reward=2172.95 +/- 3.97\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2190000, episode_reward=2377.44 +/- 3.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2195000, episode_reward=2414.07 +/- 3.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2200000, episode_reward=2255.15 +/- 7.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2205000, episode_reward=2543.19 +/- 80.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2210000, episode_reward=2616.92 +/- 16.13\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2215000, episode_reward=2597.94 +/- 55.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2220000, episode_reward=2502.71 +/- 3.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2225000, episode_reward=2494.82 +/- 8.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2230000, episode_reward=2479.00 +/- 4.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2235000, episode_reward=2498.04 +/- 4.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2240000, episode_reward=2484.74 +/- 10.99\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2245000, episode_reward=2482.55 +/- 13.05\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2250000, episode_reward=2443.38 +/- 20.60\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2255000, episode_reward=2484.52 +/- 11.73\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2260000, episode_reward=2592.97 +/- 56.37\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2265000, episode_reward=2378.96 +/- 31.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2270000, episode_reward=2387.45 +/- 6.72\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2275000, episode_reward=2435.69 +/- 16.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2280000, episode_reward=2258.28 +/- 1.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2285000, episode_reward=2206.94 +/- 2.57\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2290000, episode_reward=2303.68 +/- 3.54\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2295000, episode_reward=2296.43 +/- 5.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2300000, episode_reward=2292.72 +/- 7.35\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2305000, episode_reward=2198.58 +/- 3.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2310000, episode_reward=2290.13 +/- 3.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2315000, episode_reward=2286.32 +/- 8.10\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2320000, episode_reward=2350.11 +/- 4.14\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2325000, episode_reward=2245.88 +/- 8.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2330000, episode_reward=2252.89 +/- 3.29\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2335000, episode_reward=2296.97 +/- 5.38\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2340000, episode_reward=2304.41 +/- 5.45\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2345000, episode_reward=2305.80 +/- 3.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2350000, episode_reward=2206.79 +/- 5.75\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2355000, episode_reward=2214.16 +/- 2.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2360000, episode_reward=1914.39 +/- 1.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2365000, episode_reward=1997.56 +/- 13.65\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2370000, episode_reward=2000.42 +/- 4.02\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2375000, episode_reward=2080.42 +/- 5.50\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2380000, episode_reward=2092.01 +/- 2.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2385000, episode_reward=2083.40 +/- 4.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2390000, episode_reward=2081.49 +/- 6.55\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2395000, episode_reward=2235.83 +/- 14.04\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2400000, episode_reward=2323.44 +/- 6.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2405000, episode_reward=2324.95 +/- 4.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2410000, episode_reward=2326.09 +/- 2.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2415000, episode_reward=2329.13 +/- 3.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2420000, episode_reward=2190.37 +/- 4.85\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2425000, episode_reward=2478.13 +/- 6.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2430000, episode_reward=2169.13 +/- 3.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2435000, episode_reward=2290.98 +/- 4.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2440000, episode_reward=2400.08 +/- 10.33\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2445000, episode_reward=2134.06 +/- 1.66\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2450000, episode_reward=2032.48 +/- 16.51\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2455000, episode_reward=2058.03 +/- 6.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2460000, episode_reward=2050.45 +/- 7.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2465000, episode_reward=2056.65 +/- 6.79\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2470000, episode_reward=2121.22 +/- 12.22\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2475000, episode_reward=2455.19 +/- 12.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2480000, episode_reward=2380.15 +/- 12.80\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2485000, episode_reward=2328.50 +/- 2.31\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2490000, episode_reward=2433.98 +/- 1.64\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2495000, episode_reward=2424.87 +/- 12.48\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=2500000, episode_reward=2352.31 +/- 23.61\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='A2C', name='default', n_envs=1, timesteps=2.5e6, eval_freq=5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new directory ./data/NetworkManagement-v1-100/SAC/default/\n",
      "Eval num_timesteps=5000, episode_reward=-658057.47 +/- 69.87\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-658020.77 +/- 118.59\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=15000, episode_reward=-657981.66 +/- 90.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-658044.68 +/- 148.09\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-49331.48 +/- 220.61\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-12364.71 +/- 4478.47\n",
      "Episode length: 100.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=35000, episode_reward=-1371533.47 +/- 3.39\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-1371538.01 +/- 2.86\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-1371535.99 +/- 2.24\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-1371532.97 +/- 4.91\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=55000, episode_reward=-1371534.39 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-1371534.41 +/- 1.08\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=65000, episode_reward=-1371538.61 +/- 4.28\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=70000, episode_reward=-1371532.19 +/- 5.11\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=75000, episode_reward=-1371536.09 +/- 5.62\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-1371532.52 +/- 2.32\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=85000, episode_reward=-1371533.43 +/- 4.20\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=90000, episode_reward=-1371534.29 +/- 2.81\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=95000, episode_reward=-1371539.35 +/- 2.07\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-1371538.34 +/- 1.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=105000, episode_reward=-1371532.76 +/- 3.19\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=110000, episode_reward=-1371539.11 +/- 5.30\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=115000, episode_reward=-1371530.44 +/- 3.17\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-1371535.13 +/- 3.67\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=125000, episode_reward=-1371532.11 +/- 4.01\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=130000, episode_reward=-1371535.55 +/- 4.25\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=135000, episode_reward=-1371536.96 +/- 2.94\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-1371535.76 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=145000, episode_reward=-1371536.63 +/- 2.71\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=150000, episode_reward=-1371535.01 +/- 3.16\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=155000, episode_reward=-1371536.19 +/- 5.53\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-1371535.53 +/- 3.74\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=165000, episode_reward=-1371538.01 +/- 3.89\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=170000, episode_reward=-1371536.13 +/- 6.42\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=175000, episode_reward=-1371536.64 +/- 2.84\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-1371539.87 +/- 3.44\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=185000, episode_reward=-1371534.19 +/- 3.58\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=190000, episode_reward=-1371538.35 +/- 4.63\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=195000, episode_reward=-1371538.84 +/- 3.98\n",
      "Episode length: 100.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1371538.69 +/- 3.11\n",
      "Episode length: 100.00 +/- 0.00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 11)) of distribution Normal(loc: torch.Size([1, 11]), scale: torch.Size([1, 11])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model_on_env(env_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNetworkManagement-v1-100\u001b[39;49m\u001b[39m'\u001b[39;49m, algo_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSAC\u001b[39;49m\u001b[39m'\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdefault\u001b[39;49m\u001b[39m'\u001b[39;49m, n_envs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, eval_freq\u001b[39m=\u001b[39;49m\u001b[39m5e3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mtrain_model_on_env\u001b[0;34m(env_name, algo_name, name, n_envs, timesteps, eval_freq, env_seed)\u001b[0m\n\u001b[1;32m     44\u001b[0m model \u001b[39m=\u001b[39m make_model(algo_name, env, n_steps\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods, batch_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mnum_periods\u001b[39m*\u001b[39mn_envs)\n\u001b[1;32m     46\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(env, best_model_save_path\u001b[39m=\u001b[39msave_path, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, log_path\u001b[39m=\u001b[39msave_path, \n\u001b[1;32m     47\u001b[0m                                 eval_freq\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(eval_freq), deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 49\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39mint\u001b[39;49m(timesteps), callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/sac.py:299\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    290\u001b[0m     \u001b[39mself\u001b[39m: SelfSAC,\n\u001b[1;32m    291\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 299\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    300\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    301\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    302\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    303\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    304\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    305\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    306\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:334\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    331\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    333\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 334\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    335\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    336\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    337\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    338\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    339\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    340\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    341\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    342\u001b[0m     )\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:564\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor\u001b[39m.\u001b[39mreset_noise(env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    563\u001b[0m \u001b[39m# Select action randomly or according to policy\u001b[39;00m\n\u001b[0;32m--> 564\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39;49mnum_envs)\n\u001b[1;32m    566\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m    567\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(actions)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:395\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm._sample_action\u001b[0;34m(self, learning_starts, action_noise, n_envs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     unscaled_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_envs)])\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39m# Note: when using continuous actions,\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# we assume that the policy uses tanh to scale the action\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[39m# We use non-deterministic action in the case of SAC, for TD3, it does not matter\u001b[39;00m\n\u001b[0;32m--> 395\u001b[0m     unscaled_action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_last_obs, deterministic\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    397\u001b[0m \u001b[39m# Rescale the action from [low, high] to [-1, 1]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space, spaces\u001b[39m.\u001b[39mBox):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:535\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     observation: Union[np\u001b[39m.\u001b[39mndarray, Dict[\u001b[39mstr\u001b[39m, np\u001b[39m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    521\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    522\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/policies.py:343\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    340\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    342\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 343\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    344\u001b[0m \u001b[39m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    345\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/policies.py:341\u001b[0m, in \u001b[0;36mSACPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor(observation, deterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/sac/policies.py:168\u001b[0m, in \u001b[0;36mActor.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    166\u001b[0m mean_actions, log_std, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_action_dist_params(obs)\n\u001b[1;32m    167\u001b[0m \u001b[39m# Note: the action is squashed\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dist\u001b[39m.\u001b[39;49mactions_from_params(mean_actions, log_std, deterministic\u001b[39m=\u001b[39;49mdeterministic, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:190\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.actions_from_params\u001b[0;34m(self, mean_actions, log_std, deterministic)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mactions_from_params\u001b[39m(\u001b[39mself\u001b[39m, mean_actions: th\u001b[39m.\u001b[39mTensor, log_std: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    189\u001b[0m     \u001b[39m# Update the proba distribution\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, log_std)\n\u001b[1;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:224\u001b[0m, in \u001b[0;36mSquashedDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mproba_distribution\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[39mself\u001b[39m: SelfSquashedDiagGaussianDistribution, mean_actions: th\u001b[39m.\u001b[39mTensor, log_std: th\u001b[39m.\u001b[39mTensor\n\u001b[1;32m    223\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfSquashedDiagGaussianDistribution:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, log_std)\n\u001b[1;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:164\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m action_std \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones_like(mean_actions) \u001b[39m*\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/normal.py:56\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 56\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/torch/distributions/distribution.py:56\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     55\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 56\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m             )\n\u001b[1;32m     63\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 11)) of distribution Normal(loc: torch.Size([1, 11]), scale: torch.Size([1, 11])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])"
     ]
    }
   ],
   "source": [
    "train_model_on_env(env_name='NetworkManagement-v1-100', algo_name='SAC', name='default', n_envs=1, timesteps=1e6, eval_freq=5e3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
