{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def base_stock_policy(policy, env):\n",
    "    '''\n",
    "    Implements a re-order up-to policy. This means that for\n",
    "    each node in the network, if the inventory at that node \n",
    "    falls below the level denoted by the policy, we will \n",
    "    re-order inventory to bring it to the policy level.\n",
    "\n",
    "    For example, policy at a node is 10, current inventory\n",
    "    is 5: the action is to order 5 units.\n",
    "    '''\n",
    "\n",
    "    # Get echelon inventory levels\n",
    "    if env.period == 0:\n",
    "        inv_ech = np.cumsum(env.I[env.period] + env.T[env.period])\n",
    "    else:\n",
    "        inv_ech = np.cumsum(env.I[env.period] + env.T[env.period] - env.B[env.period-1, :-1])\n",
    "        \n",
    "    # Get unconstrained actions\n",
    "    unc_actions = policy - inv_ech\n",
    "    unc_actions = np.where(unc_actions>0, unc_actions, 0)\n",
    "\n",
    "    # Ensure that actions can be fulfilled by checking \n",
    "    # constraints\n",
    "    inv_const = np.hstack([env.I[env.period, 1:], np.Inf])\n",
    "    actions = np.minimum(env.c, np.minimum(unc_actions, inv_const))\n",
    "    return actions\n",
    "\n",
    "\n",
    "def dfo_func(policy, env, *args):\n",
    "    '''\n",
    "    Runs an episode based on current base-stock model \n",
    "    settings. This allows us to use our environment for the \n",
    "    DFO optimizer.\n",
    "    '''\n",
    "    env.reset() # Ensure env is fresh\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards = np.array(rewards)\n",
    "    prob = env.demand_dist.pmf(env.D, **env.dist_param)\n",
    "    \n",
    "    # Return negative of expected profit\n",
    "    return -1 / env.num_periods * np.sum(prob * rewards)\n",
    "\n",
    "\n",
    "def optimize_inventory_policy(env, fun, init_policy=None, method='Powell'):\n",
    "  \n",
    "    if init_policy is None:\n",
    "        init_policy = np.ones(env.num_stages-1)\n",
    "      \n",
    "    # Optimize policy\n",
    "    optim_result = minimize(fun=fun, x0=init_policy, args=env, method=method)\n",
    "    policy = optim_result.x.copy()\n",
    "  \n",
    "    # Policy must be positive integer\n",
    "    policy = np.round(np.maximum(policy, 0), 0).astype(int)\n",
    "    \n",
    "    return policy, optim_result\n",
    "\n",
    "\n",
    "def eval_baseline_policy_on_env(env_name, n_eval_episodes):\n",
    "    env = or_gym.make(env_name)\n",
    "\n",
    "    # Get the optimal baseline policy\n",
    "    policy, optim_result = optimize_inventory_policy(env, dfo_func)\n",
    "\n",
    "    print('Reorder levels:', policy)\n",
    "    print('Optimizer info:')\n",
    "    print(optim_result)\n",
    "\n",
    "    rewards = np.empty(shape=(n_eval_episodes, env.periods))\n",
    "    \n",
    "    for episode in range(n_eval_episodes):\n",
    "        env.reset()\n",
    "\n",
    "        for timestep in range(env.periods):\n",
    "            action = base_stock_policy(policy, env)\n",
    "            obs, reward, _, _ = env.step(action)\n",
    "            rewards[episode,timestep] = reward\n",
    "\n",
    "    save_path = f'./data/{env_name}/baseline/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    np.save(save_path + 'eval_rewards.npy', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorder levels: [115  10  28]\n",
      "Optimizer info:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -0.24102348149186117\n",
      "       x: [ 1.149e+02  9.862e+00  2.838e+01]\n",
      "     nit: 3\n",
      "   direc: [[ 0.000e+00  0.000e+00  1.000e+00]\n",
      "           [ 0.000e+00  1.000e+00  0.000e+00]\n",
      "           [ 1.479e+01  8.220e-01  3.302e+00]]\n",
      "    nfev: 124\n"
     ]
    }
   ],
   "source": [
    "eval_baseline_policy_on_env('InvManagement-v1', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NetInvMgmtLostSalesEnv' object has no attribute 'num_stages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_baseline_policy_on_env(\u001b[39m'\u001b[39;49m\u001b[39mNetworkManagement-v1\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[5], line 78\u001b[0m, in \u001b[0;36meval_baseline_policy_on_env\u001b[0;34m(env_name, n_eval_episodes)\u001b[0m\n\u001b[1;32m     75\u001b[0m env \u001b[39m=\u001b[39m or_gym\u001b[39m.\u001b[39mmake(env_name)\n\u001b[1;32m     77\u001b[0m \u001b[39m# Get the optimal baseline policy\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m policy, optim_result \u001b[39m=\u001b[39m optimize_inventory_policy(env, dfo_func)\n\u001b[1;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mReorder levels:\u001b[39m\u001b[39m'\u001b[39m, policy)\n\u001b[1;32m     81\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mOptimizer info:\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36moptimize_inventory_policy\u001b[0;34m(env, fun, init_policy, method)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize_inventory_policy\u001b[39m(env, fun, init_policy\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPowell\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[39mif\u001b[39;00m init_policy \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         init_policy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(env\u001b[39m.\u001b[39;49mnum_stages\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Optimize policy\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     optim_result \u001b[39m=\u001b[39m minimize(fun\u001b[39m=\u001b[39mfun, x0\u001b[39m=\u001b[39minit_policy, args\u001b[39m=\u001b[39menv, method\u001b[39m=\u001b[39mmethod)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/gym/core.py:238\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mattempted to get missing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name)\n\u001b[1;32m    237\u001b[0m     )\n\u001b[0;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NetInvMgmtLostSalesEnv' object has no attribute 'num_stages'"
     ]
    }
   ],
   "source": [
    "eval_baseline_policy_on_env('NetworkManagement-v1', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec 15 2022, 10:44:50) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
