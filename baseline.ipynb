{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def base_stock_policy(policy, env):\n",
    "    '''\n",
    "    Implements a re-order up-to policy. This means that for\n",
    "    each node in the network, if the inventory at that node \n",
    "    falls below the level denoted by the policy, we will \n",
    "    re-order inventory to bring it to the policy level.\n",
    "\n",
    "    For example, policy at a node is 10, current inventory\n",
    "    is 5: the action is to order 5 units.\n",
    "    '''\n",
    "\n",
    "    # Get echelon inventory levels\n",
    "    if env.period == 0:\n",
    "        inv_ech = np.cumsum(env.I[env.period] + env.T[env.period])\n",
    "    else:\n",
    "        inv_ech = np.cumsum(env.I[env.period] + env.T[env.period] - env.B[env.period-1, :-1])\n",
    "        \n",
    "    # Get unconstrained actions\n",
    "    unc_actions = policy - inv_ech\n",
    "    unc_actions = np.where(unc_actions>0, unc_actions, 0)\n",
    "\n",
    "    # Ensure that actions can be fulfilled by checking \n",
    "    # constraints\n",
    "    inv_const = np.hstack([env.I[env.period, 1:], np.Inf])\n",
    "    actions = np.minimum(env.c, np.minimum(unc_actions, inv_const))\n",
    "    return actions\n",
    "\n",
    "\n",
    "def dfo_func(policy, env, *args):\n",
    "    '''\n",
    "    Runs an episode based on current base-stock model \n",
    "    settings. This allows us to use our environment for the \n",
    "    DFO optimizer.\n",
    "    '''\n",
    "    env.reset() # Ensure env is fresh\n",
    "    rewards = []\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = base_stock_policy(policy, env)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    rewards = np.array(rewards)\n",
    "    prob = env.demand_dist.pmf(env.D, **env.dist_param)\n",
    "    \n",
    "    # Return negative of expected profit\n",
    "    return -1 / env.num_periods * np.sum(prob * rewards)\n",
    "\n",
    "\n",
    "def optimize_inventory_policy(env, fun, init_policy=None, method='Powell'):\n",
    "  \n",
    "    if init_policy is None:\n",
    "        init_policy = np.ones(env.num_stages-1)\n",
    "      \n",
    "    # Optimize policy\n",
    "    optim_result = minimize(fun=fun, x0=init_policy, args=env, method=method)\n",
    "    policy = optim_result.x.copy()\n",
    "  \n",
    "    # Policy must be positive integer\n",
    "    policy = np.round(np.maximum(policy, 0), 0).astype(int)\n",
    "    \n",
    "    return policy, optim_result\n",
    "\n",
    "\n",
    "def eval_baseline_policy_on_env(env_name, n_eval_episodes):\n",
    "    env = or_gym.make(env_name)\n",
    "\n",
    "    # Get the optimal baseline policy\n",
    "    policy, optim_result = optimize_inventory_policy(env, dfo_func)\n",
    "\n",
    "    print('Reorder levels:', policy)\n",
    "    print('Optimizer info:')\n",
    "    print(optim_result)\n",
    "\n",
    "    rewards = np.empty(shape=(n_eval_episodes, env.periods))\n",
    "    \n",
    "    for episode in range(n_eval_episodes):\n",
    "        env.reset()\n",
    "\n",
    "        for timestep in range(env.periods):\n",
    "            action = base_stock_policy(policy, env)\n",
    "            obs, reward, _, _ = env.step(action)\n",
    "            rewards[episode,timestep] = reward\n",
    "\n",
    "    save_path = f'./data/{env_name}/baseline/'\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    np.save(save_path + 'eval_rewards.npy', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reorder levels: [115  10  28]\n",
      "Optimizer info:\n",
      " message: Optimization terminated successfully.\n",
      " success: True\n",
      "  status: 0\n",
      "     fun: -0.24102348149186117\n",
      "       x: [ 1.149e+02  9.862e+00  2.838e+01]\n",
      "     nit: 3\n",
      "   direc: [[ 0.000e+00  0.000e+00  1.000e+00]\n",
      "           [ 0.000e+00  1.000e+00  0.000e+00]\n",
      "           [ 1.479e+01  8.220e-01  3.302e+00]]\n",
      "    nfev: 124\n"
     ]
    }
   ],
   "source": [
    "eval_baseline_policy_on_env('InvManagement-v1', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
