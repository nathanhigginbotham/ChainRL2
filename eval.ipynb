{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import or_gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.sac.policies import MlpPolicy as SACPolicy\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.a2c.policies import MlpPolicy as A2CPolicy\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import MlpPolicy as PPOPolicy\n",
    "\n",
    "from sb3_contrib import ARS\n",
    "from sb3_contrib.ars.policies import ARSPolicy\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from sb3_contrib.ppo_recurrent.policies import RecurrentActorCriticPolicy\n",
    "\n",
    "from sb3_contrib import TQC\n",
    "from sb3_contrib.tqc.policies import MlpPolicy as TQCPolicy\n",
    "\n",
    "from sb3_contrib import TRPO\n",
    "from sb3_contrib.trpo.policies import MlpPolicy as TRPOPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_on_env(model, env, n_eval_episodes):\n",
    "    rewards = np.empty(shape=(n_eval_episodes, env.periods))\n",
    "    obs = env.reset()\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        for timestep in range(env.periods):\n",
    "            action = model.predict(obs)\n",
    "            obs, reward, _, _ = env.step(action[0])\n",
    "            rewards[episode,timestep] = reward \n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def eval_best_model(env_name, algorithm, name, n_eval_episodes):\n",
    "    env = or_gym.make(env_name)\n",
    "    model = algorithm.load(f'./data/{env_name}/{algorithm.__name__}/{name}/best_model', env=env)\n",
    "    rewards = eval_model_on_env(model, env, n_eval_episodes)\n",
    "\n",
    "    np.save(f'./data/{env_name}/{algorithm.__name__}/{name}/best_model_eval_rewards.npy', rewards) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_best_model('InvManagement-v1', PPO, 'default', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_best_model('InvManagement-v1', A2C, 'default', 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
